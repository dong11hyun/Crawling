"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ DAG (v4 - Kafka ì—°ë™)
- í¬ë¡¤ë§ â†’ Kafka ë°œí–‰
- Consumerê°€ ë³„ë„ë¡œ PostgreSQL/OpenSearch ì €ì¥
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import sys
import logging

# src í´ë”ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, '/opt/airflow/src')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ========================================
# DAG ê¸°ë³¸ ì„¤ì •
# ========================================
default_args = {
    'owner': 'musinsa-team',
    'depends_on_past': False,
    'start_date': datetime(2026, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
}

dag = DAG(
    'musinsa_kafka_dag',
    default_args=default_args,
    description='ë¬´ì‹ ì‚¬ ìƒí’ˆ ìˆ˜ì§‘ â†’ Kafka ë°œí–‰ (v4)',
    schedule_interval='0 6 * * *',
    catchup=False,
    max_active_runs=1,
    tags=['musinsa', 'crawling', 'kafka', 'production'],
)


# ========================================
# Task í•¨ìˆ˜ë“¤
# ========================================
def run_crawl_task(**context):
    """
    í¬ë¡¤ë§ Task
    """
    from tasks.crawl_task import crawl_musinsa
    from tasks.xcom_utils import should_use_file, save_to_file
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    data = crawl_musinsa(
        keyword="íŒ¨ë”©",
        scroll_count=2,
        max_products=15
    )
    
    ti = context['ti']
    
    # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ì „ë‹¬ ë°©ì‹ ê²°ì •
    if should_use_file(data):
        filepath = save_to_file(data, prefix="crawl")
        ti.xcom_push(key='crawled_data_path', value=filepath)
        ti.xcom_push(key='use_file', value=True)
    else:
        ti.xcom_push(key='crawled_data', value=data)
        ti.xcom_push(key='use_file', value=False)
    
    logger.info(f"ğŸ“¦ í¬ë¡¤ë§ ì™„ë£Œ: {len(data)}ê±´")
    return len(data)


def run_validate_task(**context):
    """
    ê²€ì¦ Task
    """
    from tasks.validate_task import validate_products
    from tasks.xcom_utils import load_from_file, should_use_file, save_to_file
    
    ti = context['ti']
    use_file = ti.xcom_pull(key='use_file', task_ids='crawl_task')
    
    if use_file:
        filepath = ti.xcom_pull(key='crawled_data_path', task_ids='crawl_task')
        data = load_from_file(filepath)
    else:
        data = ti.xcom_pull(key='crawled_data', task_ids='crawl_task')
    
    if not data:
        raise ValueError("í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    valid_data, invalid_count = validate_products(data)
    
    if should_use_file(valid_data):
        filepath = save_to_file(valid_data, prefix="valid")
        ti.xcom_push(key='valid_data_path', value=filepath)
        ti.xcom_push(key='valid_use_file', value=True)
    else:
        ti.xcom_push(key='valid_data', value=valid_data)
        ti.xcom_push(key='valid_use_file', value=False)
    
    logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(valid_data)}ê±´ í†µê³¼, {invalid_count}ê±´ ì‹¤íŒ¨")
    return len(valid_data)


def run_publish_to_kafka(**context):
    """
    Kafka ë°œí–‰ Task (ì‹ ê·œ!)
    - ê²€ì¦ëœ ë°ì´í„°ë¥¼ Kafkaë¡œ ë°œí–‰
    - Consumerê°€ PostgreSQL/OpenSearchì— ì €ì¥
    """
    import json
    from kafka import KafkaProducer
    from tasks.xcom_utils import load_from_file
    from datetime import datetime, timezone, timedelta
    
    KST = timezone(timedelta(hours=9))
    
    ti = context['ti']
    use_file = ti.xcom_pull(key='valid_use_file', task_ids='validate_task')
    
    if use_file:
        filepath = ti.xcom_pull(key='valid_data_path', task_ids='validate_task')
        data = load_from_file(filepath)
    else:
        data = ti.xcom_pull(key='valid_data', task_ids='validate_task')
    
    if not data:
        logger.warning("ë°œí–‰í•  ë°ì´í„° ì—†ìŒ")
        return {"success": 0, "failed": 0}
    
    # Kafka Producer ìƒì„± (Docker ë‚´ë¶€ì—ì„œëŠ” kafka:29092 ì‚¬ìš©)
    producer = KafkaProducer(
        bootstrap_servers='musinsa-kafka:29092',
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),
        key_serializer=lambda k: k.encode('utf-8') if k else None,
        acks='all',
        retries=3,
    )
    
    success = 0
    failed = 0
    
    for item in data:
        try:
            item['published_at'] = datetime.now(KST).isoformat()
            key = item.get('url', 'unknown')
            
            future = producer.send('musinsa-products', key=key, value=item)
            future.get(timeout=10)
            success += 1
            
        except Exception as e:
            logger.error(f"âŒ ë°œí–‰ ì‹¤íŒ¨: {e}")
            failed += 1
    
    producer.flush()
    producer.close()
    
    logger.info(f"ğŸ“¤ Kafka ë°œí–‰ ì™„ë£Œ: ì„±ê³µ {success}, ì‹¤íŒ¨ {failed}")
    return {"success": success, "failed": failed}


def run_cleanup_task(**context):
    """
    ì •ë¦¬ Task
    """
    from tasks.xcom_utils import cleanup_old_files
    
    deleted = cleanup_old_files(days=7)
    return deleted


# ========================================
# Task ì •ì˜
# ========================================
crawl_task = PythonOperator(
    task_id='crawl_task',
    python_callable=run_crawl_task,
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_task',
    python_callable=run_validate_task,
    dag=dag,
)

publish_kafka_task = PythonOperator(
    task_id='publish_kafka_task',
    python_callable=run_publish_to_kafka,
    dag=dag,
)

cleanup_task = PythonOperator(
    task_id='cleanup_task',
    python_callable=run_cleanup_task,
    dag=dag,
)

# ========================================
# Task ì˜ì¡´ì„±
# ========================================
crawl_task >> validate_task >> publish_kafka_task >> cleanup_task
