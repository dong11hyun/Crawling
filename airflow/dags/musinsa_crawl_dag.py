"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ DAG
- ë§¤ì¼ ì˜¤ì „ 6ì‹œ ìë™ ì‹¤í–‰
- í¬ë¡¤ë§ â†’ ê²€ì¦ â†’ PostgreSQL/OpenSearch ì €ì¥
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import os
import sys

# src í´ë”ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, '/opt/airflow/src')

# ========================================
# DAG ê¸°ë³¸ ì„¤ì •
# ========================================
default_args = {
    'owner': 'musinsa-team',
    'depends_on_past': False,
    'start_date': datetime(2026, 1, 1),
    'retries': 3,                          # ì‹¤íŒ¨ ì‹œ 3ë²ˆ ì¬ì‹œë„
    'retry_delay': timedelta(minutes=5),   # ì¬ì‹œë„ ê°„ê²© 5ë¶„
    'retry_exponential_backoff': True,     # ì¬ì‹œë„ ê°„ê²© ì ì  ëŠ˜ë¦¬ê¸°
    'max_retry_delay': timedelta(minutes=30),
    'email_on_failure': False,             # ì´ë©”ì¼ ì•Œë¦¼ (ì„¤ì • ì‹œ True)
    'email_on_retry': False,
}

dag = DAG(
    'musinsa_crawl_dag',
    default_args=default_args,
    description='ë¬´ì‹ ì‚¬ ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 6 * * *',  # ë§¤ì¼ ì˜¤ì „ 6ì‹œ (KST ê¸°ì¤€ ì¡°ì • í•„ìš”)
    catchup=False,                  # ê³¼ê±° ì‹¤í–‰ ê±´ë„ˆë›°ê¸°
    max_active_runs=1,              # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
    tags=['musinsa', 'crawling', 'production'],
)


# ========================================
# Task 1: í¬ë¡¤ë§ ì‹¤í–‰
# ========================================
def run_crawler(**context):
    """
    ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    - Playwright ë¹„ë™ê¸° í¬ë¡¤ë§
    - ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘
    """
    import asyncio
    from playwright.async_api import async_playwright
    from opensearchpy import OpenSearch, helpers
    from datetime import datetime
    import re
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
    OPENSEARCH_HOST = os.getenv('OPENSEARCH_HOST', 'localhost')
    OPENSEARCH_PORT = int(os.getenv('OPENSEARCH_PORT', 9201))
    
    SEARCH_KEYWORD = "íŒ¨ë”©"
    TARGET_URL = f"https://www.musinsa.com/search/goods?keyword={SEARCH_KEYWORD}&gf=A"
    SCROLL_COUNT = 2  # Airflowì—ì„œëŠ” ì ê²Œ ì„¤ì •
    INDEX_NAME = "musinsa_products"
    
    # OpenSearch í´ë¼ì´ì–¸íŠ¸
    opensearch_client = OpenSearch(
        hosts=[{'host': OPENSEARCH_HOST, 'port': OPENSEARCH_PORT}],
        http_compress=True, use_ssl=False, verify_certs=False
    )
    
    async def crawl():
        collected_data = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)  # headless ëª¨ë“œ
            context = await browser.new_context()
            page = await context.new_page()
            
            print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {SEARCH_KEYWORD}")
            await page.goto(TARGET_URL, timeout=60000)
            
            # ìŠ¤í¬ë¡¤
            for i in range(SCROLL_COUNT):
                await page.keyboard.press("End")
                await asyncio.sleep(2)
            
            # ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
            locators = page.locator("a[href*='/products/']")
            count = await locators.count()
            urls = set()
            for i in range(min(count, 10)):  # í…ŒìŠ¤íŠ¸ìš© 10ê°œ
                href = await locators.nth(i).get_attribute("href")
                if href:
                    if not href.startswith("http"):
                        href = "https://www.musinsa.com" + href
                    urls.add(href)
            
            url_list = list(urls)
            print(f"   ìˆ˜ì§‘ ëŒ€ìƒ: {len(url_list)}ê°œ")
            
            for url in url_list:
                try:
                    await page.goto(url, timeout=30000)
                    await page.wait_for_load_state("domcontentloaded")
                    
                    # ë°ì´í„° ì¶”ì¶œ
                    title_loc = page.locator("meta[property='og:title']").first
                    title = await title_loc.get_attribute("content") if await title_loc.count() > 0 else "ì œëª©ì—†ìŒ"
                    
                    brand_loc = page.locator("meta[property='product:brand']").first
                    brand = await brand_loc.get_attribute("content") if await brand_loc.count() > 0 else ""
                    
                    price_loc = page.locator("meta[property='product:price:amount']").first
                    if await price_loc.count() > 0:
                        price_text = await price_loc.get_attribute("content")
                        price_num = re.sub(r'[^0-9]', '', str(price_text))
                        price = int(price_num) if price_num else 0
                    else:
                        price = 0
                    
                    collected_data.append({
                        "url": url,
                        "title": title,
                        "brand": brand,
                        "price": price,
                        "seller_info": {},
                        "crawled_at": datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    print(f"   âŒ ì—ëŸ¬: {e}")
                    continue
            
            await browser.close()
        
        return collected_data
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    data = asyncio.run(crawl())
    
    # XComìœ¼ë¡œ ë‹¤ìŒ Taskì— ì „ë‹¬
    context['ti'].xcom_push(key='crawled_data', value=data)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(data)}ê±´")
    
    return len(data)


# ========================================
# Task 2: ë°ì´í„° ê²€ì¦
# ========================================
def validate_data(**context):
    """
    í¬ë¡¤ë§ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    - í•„ìˆ˜ í•„ë“œ í™•ì¸
    - ê°€ê²© ë²”ìœ„ í™•ì¸
    """
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    data = ti.xcom_pull(key='crawled_data', task_ids='crawl_task')
    
    if not data:
        raise ValueError("âŒ í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    valid_data = []
    invalid_count = 0
    
    for item in data:
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not item.get('url') or not item.get('title'):
            invalid_count += 1
            continue
        
        # ê°€ê²© ë²”ìœ„ ê²€ì¦ (ìŒìˆ˜ ë°©ì§€)
        if item.get('price', 0) < 0:
            item['price'] = 0
        
        valid_data.append(item)
    
    print(f"âœ… ê²€ì¦ ì™„ë£Œ: ìœ íš¨ {len(valid_data)}ê±´, ë¬´íš¨ {invalid_count}ê±´")
    
    # ë‹¤ìŒ Taskë¡œ ì „ë‹¬
    ti.xcom_push(key='valid_data', value=valid_data)
    
    return len(valid_data)


# ========================================
# Task 3: ë°ì´í„° ì €ì¥
# ========================================
def load_data(**context):
    """
    ê²€ì¦ëœ ë°ì´í„°ë¥¼ PostgreSQL + OpenSearchì— ì €ì¥
    """
    from opensearchpy import OpenSearch, helpers
    from sqlalchemy import create_engine
    from sqlalchemy.orm import sessionmaker
    
    # XComì—ì„œ ê²€ì¦ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    data = ti.xcom_pull(key='valid_data', task_ids='validate_task')
    
    if not data:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    # ---- PostgreSQL ì €ì¥ ----
    try:
        DB_URL = os.getenv('MUSINSA_DB_URL')
        if DB_URL:
            engine = create_engine(DB_URL)
            Session = sessionmaker(bind=engine)
            db = Session()
            
            # ê°„ë‹¨íˆ raw SQLë¡œ ì €ì¥ (models import ì´ìŠˆ íšŒí”¼)
            for item in data:
                db.execute(
                    """
                    INSERT INTO products (url, title, brand, price, created_at, updated_at)
                    VALUES (:url, :title, :brand, :price, NOW(), NOW())
                    ON CONFLICT (url) DO UPDATE SET
                        title = EXCLUDED.title,
                        brand = EXCLUDED.brand,
                        price = EXCLUDED.price,
                        updated_at = NOW()
                    """,
                    item
                )
            db.commit()
            db.close()
            print(f"   ğŸ’¾ PostgreSQL ì €ì¥ ì™„ë£Œ: {len(data)}ê±´")
    except Exception as e:
        print(f"   âš ï¸ PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ---- OpenSearch ì €ì¥ ----
    try:
        OPENSEARCH_HOST = os.getenv('OPENSEARCH_HOST', 'localhost')
        OPENSEARCH_PORT = int(os.getenv('OPENSEARCH_PORT', 9201))
        
        client = OpenSearch(
            hosts=[{'host': OPENSEARCH_HOST, 'port': OPENSEARCH_PORT}],
            http_compress=True, use_ssl=False, verify_certs=False
        )
        
        docs = [{"_index": "musinsa_products", "_source": item} for item in data]
        success, failed = helpers.bulk(client, docs)
        print(f"   ğŸ” OpenSearch ì €ì¥ ì™„ë£Œ: ì„±ê³µ {success}, ì‹¤íŒ¨ {failed}")
    except Exception as e:
        print(f"   âš ï¸ OpenSearch ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return len(data)


# ========================================
# Task ì •ì˜
# ========================================
crawl_task = PythonOperator(
    task_id='crawl_task',
    python_callable=run_crawler,
    dag=dag,
)

validate_task = PythonOperator(
    task_id='validate_task',
    python_callable=validate_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_task',
    python_callable=load_data,
    dag=dag,
)

# ========================================
# Task ì˜ì¡´ì„± (ìˆœì„œ)
# ========================================
crawl_task >> validate_task >> load_task
