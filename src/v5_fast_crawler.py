"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v5 - ë³‘ë ¬ ìˆ˜ì§‘ (ThreadPoolExecutor)
- v4ì˜ ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€
- ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬ (max_workers=4)
- ì˜ˆìƒ ì†ë„: v4 ëŒ€ë¹„ 3~4ë°° í–¥ìƒ
"""
import requests
from bs4 import BeautifulSoup
import time
import json
import random
import os
import logging
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
from opensearchpy import OpenSearch, helpers
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ================= ì„¤ì • =================
SEARCH_KEYWORD = "íŒ¨ë”©"
MAX_PRODUCTS = 1000
BATCH_SIZE = 60
MAX_WORKERS = 2  # ë™ì‹œ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (2ê°œë¡œ ì¶•ì†Œ - ì°¨ë‹¨ ë°©ì§€)

# ë”œë ˆì´ ì„¤ì • (ì´ˆ) - ê° ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
API_DELAY = 0.8
HTML_DELAY = 1.5  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ê°œë³„ ì›Œì»¤ ë”œë ˆì´ (ë” ëŠë¦¬ê²Œ)
RANDOM_RANGE = (0.3, 0.8)  # ì¶”ê°€ ëœë¤ ë”œë ˆì´ ì¦ê°€

# User-Agent ë¡œí…Œì´ì…˜
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

# íŒŒì¼ ê²½ë¡œ
PROGRESS_FILE = "data/progress_v5.json"
JSONL_FILE = "data/crawl_progress_v5_{keyword}.jsonl"
OUTPUT_FILE = "data/crawl_result_v5_{keyword}_{timestamp}.json"

# ì „ì—­ ì¤‘ì§€ í”Œë˜ê·¸
STOP_CRAWLER_FLAG = False

# Thread-safe ë½
results_lock = threading.Lock()
progress_lock = threading.Lock()

# ë¡œê±° ì„¤ì •
def setup_logger():
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    logger = logging.getLogger("MusinsaCrawlerV5")
    logger.setLevel(logging.INFO)
    
    filename = os.path.join(log_dir, "crawler_v5.log")
    file_handler = TimedRotatingFileHandler(
        filename, when="midnight", interval=1, backupCount=7, encoding="utf-8"
    )
    file_handler.suffix = "%Y-%m-%d"
    file_formatter = logging.Formatter(
        "[%(asctime)s] [%(levelname)s] [%(threadName)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )
    file_handler.setFormatter(file_formatter)
    
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter("[%(asctime)s] [%(threadName)s] %(message)s", datefmt="%H:%M:%S")
    console_handler.setFormatter(console_formatter)
    
    if not logger.handlers:
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
    return logger

logger = setup_logger()

def stop_crawling():
    global STOP_CRAWLER_FLAG
    STOP_CRAWLER_FLAG = True
    logger.warning("\nğŸ›‘ í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­ë¨!")

# ì „ì—­ ì§„í–‰ ìƒíƒœ
CRAWL_PROGRESS = {
    "status": "idle",
    "keyword": "",
    "total": 0,
    "current": 0,
    "start_time": 0.0,
    "version": "v5"
}

def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "application/json, text/html",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Referer": "https://www.musinsa.com/",
    }

def safe_delay(base_delay: float):
    delay = base_delay + random.uniform(*RANDOM_RANGE)
    time.sleep(delay)

def load_progress(keyword: str = "") -> set:
    """ì§„í–‰ ìƒíƒœ ë¡œë“œ: progress_v5.json ë˜ëŠ” JSONL íŒŒì¼ì—ì„œ ë³µêµ¬"""
    collected_ids = set()
    
    # 1. progress_v5.jsonì—ì„œ ë¡œë“œ ì‹œë„
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            collected_ids = set(data.get('collected_ids', []))
            if collected_ids:
                return collected_ids
    
    # 2. JSONL íŒŒì¼ì—ì„œ ë³µêµ¬ ì‹œë„ (progress_v5.jsonì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì„ ë•Œ)
    if keyword:
        jsonl_path = JSONL_FILE.format(keyword=keyword)
        if os.path.exists(jsonl_path):
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            item = json.loads(line)
                            if 'goodsNo' in item:
                                collected_ids.add(item['goodsNo'])
                        except:
                            pass
    
    return collected_ids

def save_progress(collected_ids: set):
    """ìˆ˜ì§‘ëœ ID ëª©ë¡ì„ íŒŒì¼ì— ì €ì¥ (ì‹¤ì‹œê°„)"""
    os.makedirs('data', exist_ok=True)
    with progress_lock:
        with open(PROGRESS_FILE, 'w') as f:
            json.dump({'collected_ids': list(collected_ids)}, f)

def append_to_jsonl(filepath: str, data: dict):
    os.makedirs('data', exist_ok=True)
    with results_lock:
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(json.dumps(data, ensure_ascii=False) + '\n')

def load_jsonl(filepath: str) -> list:
    results = []
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    results.append(json.loads(line))
    return results

def get_product_list(keyword: str, page: int, size: int = 60, session=None) -> list:
    url = "https://api.musinsa.com/api2/dp/v1/plp/goods"
    params = {
        "gf": "A",
        "keyword": keyword,
        "sortCode": "POPULAR",
        "isUsed": "false",
        "page": page,
        "size": size,
        "caller": "SEARCH"
    }
    
    for attempt in range(3):
        try:
            response = session.get(url, params=params, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                logger.warning(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
            
            response.raise_for_status()
            data = response.json()
            return data.get("data", {}).get("list", [])
            
        except Exception as e:
            logger.error(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}", exc_info=True)
            time.sleep(10)
    
    return []

def get_seller_info(goods_no: int, session=None) -> dict:
    """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ íŒë§¤ì ì •ë³´ ì¶”ì¶œ (ì›Œì»¤ì—ì„œ í˜¸ì¶œ)"""
    url = f"https://www.musinsa.com/products/{goods_no}"
    
    # ê°œë³„ ì›Œì»¤ ë”œë ˆì´ (ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë¶€í•˜ ë¶„ì‚°)
    safe_delay(HTML_DELAY)
    
    for attempt in range(3):
        try:
            response = session.get(url, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                logger.warning(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
                
            response.raise_for_status()
            
            try:
                soup = BeautifulSoup(response.text, 'lxml')
            except Exception:
                soup = BeautifulSoup(response.text, 'html.parser')

            next_data_script = soup.find('script', id='__NEXT_DATA__')
            
            if not next_data_script:
                return {}
            
            data = json.loads(next_data_script.string)
            company = data.get('props', {}).get('pageProps', {}).get('meta', {}).get('data', {}).get('company', {})
            
            if not company:
                return {}
            
            return {
                "company": company.get('name', ''),
                "ceo": company.get('ceoName', ''),
                "biz_num": company.get('businessNumber', ''),
                "license": company.get('mailOrderReportNumber', ''),
                "contact": company.get('phoneNumber', ''),
                "email": company.get('email', ''),
                "address": f"{company.get('address', '')} {company.get('detailAddress', '')}".strip()
            }
            
        except Exception as e:
            logger.error(f"   âŒ HTML íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}", exc_info=True)
            time.sleep(5)
    
    return {}

def get_opensearch_client():
    return OpenSearch(
        hosts=[{'host': 'localhost', 'port': 9201}],
        http_auth=None,
        use_ssl=False,
        verify_certs=False,
        timeout=30
    )

def flush_bulk_buffer(client, buffer: list):
    if not buffer:
        return
    try:
        success, _ = helpers.bulk(client, buffer, refresh=True)
        logger.info(f"      ğŸš€ [Bulk] {len(buffer)}ê°œ ì•„ì´í…œ OpenSearch ì ì¬ ì™„ë£Œ")
        buffer.clear()
    except Exception as e:
        logger.error(f"      âŒ [Bulk] ì ì¬ ì‹¤íŒ¨: {e}", exc_info=True)

def add_to_bulk_buffer(buffer: list, data: dict, index_name: str = "musinsa_products"):
    doc_id = str(data['goodsNo'])
    if 'crawled_at' not in data:
        data['crawled_at'] = datetime.now().isoformat()
    action = {
        "_index": index_name,
        "_id": doc_id,
        "_source": data
    }
    buffer.append(action)

def process_single_product(product: dict, session, collected_ids: set, jsonl_path: str, bulk_buffer: list, os_client):
    """ë‹¨ì¼ ìƒí’ˆ ì²˜ë¦¬ (ì›Œì»¤ í•¨ìˆ˜)"""
    global STOP_CRAWLER_FLAG, CRAWL_PROGRESS
    
    if STOP_CRAWLER_FLAG:
        return None
    
    goods_no = product.get("goodsNo")
    goods_name = product.get("goodsName", "")[:30]
    
    logger.info(f"   ì²˜ë¦¬ ì¤‘: {goods_name}...")
    
    # íŒë§¤ì ì •ë³´ ì¶”ì¶œ
    seller_info = get_seller_info(goods_no, session)
    
    # ê²°ê³¼ ì¡°í•©
    result = {
        "goodsNo": goods_no,
        "title": product.get("goodsName"),
        "brand": product.get("brandName"),
        "price": product.get("price"),
        "normalPrice": product.get("normalPrice"),
        "saleRate": product.get("saleRate"),
        "url": f"https://www.musinsa.com/products/{goods_no}",
        "thumbnail": product.get("thumbnail"),
        "seller_info": seller_info,
        "crawled_at": datetime.now().isoformat()
    }
    
    # Thread-safe ì €ì¥
    append_to_jsonl(jsonl_path, result)
    
    # ìˆ˜ì§‘ ID ì¶”ê°€ ë° ì €ì¥
    with progress_lock:
        collected_ids.add(goods_no)
        CRAWL_PROGRESS["current"] = len(collected_ids)
    
    # ì§„í–‰ ìƒíƒœ íŒŒì¼ì— ì €ì¥ (10ê°œë§ˆë‹¤)
    if len(collected_ids) % 10 == 0:
        save_progress(collected_ids)
    
    # Bulk ë²„í¼ ì¶”ê°€ (Thread-safe)
    if os_client:
        with results_lock:
            add_to_bulk_buffer(bulk_buffer, result)
            if len(bulk_buffer) >= 20:
                flush_bulk_buffer(os_client, bulk_buffer)
    
    return result

def run_crawler(keyword: str = SEARCH_KEYWORD, max_products: int = MAX_PRODUCTS):
    """ë©”ì¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰ (v5 - ë³‘ë ¬ ì²˜ë¦¬)"""
    global STOP_CRAWLER_FLAG, CRAWL_PROGRESS
    STOP_CRAWLER_FLAG = False
    
    start_time = time.time()
    
    CRAWL_PROGRESS.update({
        "status": "running",
        "keyword": keyword,
        "total": max_products,
        "current": 0,
        "start_time": start_time,
        "version": "v5"
    })
    
    logger.info("=" * 60)
    logger.info(f"ğŸš€ ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v5 (ë³‘ë ¬ ìˆ˜ì§‘) ì‹œì‘")
    logger.info(f"   ê²€ìƒ‰ì–´: {keyword}")
    logger.info(f"   ëª©í‘œ: {max_products}ê°œ")
    logger.info(f"   ë³‘ë ¬ ì›Œì»¤: {MAX_WORKERS}ê°œ")
    logger.info("=" * 60)
    
    session = requests.Session()
    
    try:
        os_client = get_opensearch_client()
        if os_client.ping():
            logger.info("   âœ… OpenSearch ì—°ê²° ì„±ê³µ")
        else:
            logger.warning("   âš ï¸ OpenSearch ì—°ê²° ì‹¤íŒ¨")
            os_client = None
    except Exception as e:
        logger.error(f"   âš ï¸ OpenSearch ì´ˆê¸°í™” ì—ëŸ¬: {e}", exc_info=True)
        os_client = None
    
    collected_ids = load_progress(keyword)
    if collected_ids:
        logger.info(f"ğŸ“‚ ì´ì „ ì§„í–‰ ìƒíƒœ ë³µì›: {len(collected_ids)}ê°œ ì´ë¯¸ ìˆ˜ì§‘ë¨")
    
    # 1ë‹¨ê³„: ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘
    logger.info(f"\nğŸ” [1ë‹¨ê³„] ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ ì¤‘...")
    
    bulk_buffer = []
    all_products = []
    page = 1
    
    while len(all_products) < max_products:
        if STOP_CRAWLER_FLAG:
            logger.warning("   ğŸ›‘ [1ë‹¨ê³„] ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œ")
            CRAWL_PROGRESS["status"] = "stopped"
            break

        products = get_product_list(keyword, page, BATCH_SIZE, session)
        
        if not products:
            logger.info(f"   ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        new_products = [p for p in products if p.get('goodsNo') not in collected_ids]
        all_products.extend(new_products)
        
        logger.info(f"   í˜ì´ì§€ {page}: {len(new_products)}ê°œ ì¶”ê°€ (ì´ {len(all_products)}ê°œ)")
        
        page += 1
        safe_delay(API_DELAY)
        
        if len(all_products) >= max_products:
            all_products = all_products[:max_products]
            break
    
    logger.info(f"   âœ… ìˆ˜ì§‘í•  ìƒí’ˆ: {len(all_products)}ê°œ")
    
    # 2ë‹¨ê³„: íŒë§¤ì ì •ë³´ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
    logger.info(f"\nğŸ“¦ [2ë‹¨ê³„] íŒë§¤ì ì •ë³´ ë³‘ë ¬ ìˆ˜ì§‘ ì¤‘... (ì›Œì»¤: {MAX_WORKERS}ê°œ)")
    
    jsonl_path = JSONL_FILE.format(keyword=keyword)
    results = load_jsonl(jsonl_path)
    if results:
        logger.info(f"   ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë³µì›: {len(results)}ê°œ")
    
    CRAWL_PROGRESS["total"] = len(all_products)
    
    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(
                process_single_product, 
                product, session, collected_ids, jsonl_path, bulk_buffer, os_client
            ): product for product in all_products
        }
        
        for future in as_completed(futures):
            if STOP_CRAWLER_FLAG:
                logger.warning("   ğŸ›‘ [2ë‹¨ê³„] ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­ ê°ì§€")
                executor.shutdown(wait=False, cancel_futures=True)
                break
            
            try:
                result = future.result()
                if result:
                    results.append(result)
            except Exception as e:
                logger.error(f"   âŒ ì›Œì»¤ ì—ëŸ¬: {e}", exc_info=True)
    
    # ìµœì¢… ì €ì¥
    elapsed = time.time() - start_time
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_FILE.format(keyword=keyword, timestamp=timestamp)
    
    os.makedirs('data', exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    if os_client and bulk_buffer:
        logger.info("   ğŸ§¹ ë‚¨ì€ ë°ì´í„° Bulk ì ì¬ ì¤‘...")
        flush_bulk_buffer(os_client, bulk_buffer)
    
    # ì •ë¦¬
    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)
    if os.path.exists(jsonl_path):
        os.remove(jsonl_path)
    
    logger.info("\n" + "=" * 60)
    logger.info(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    logger.info(f"   ì´ ìˆ˜ì§‘: {len(results)}ê°œ")
    logger.info(f"   ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    logger.info(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    logger.info("=" * 60)
    
    if not STOP_CRAWLER_FLAG:
        CRAWL_PROGRESS["status"] = "finished"
        CRAWL_PROGRESS["current"] = len(results)

    return results


if __name__ == "__main__":
    import sys
    keyword = sys.argv[1] if len(sys.argv) > 1 else SEARCH_KEYWORD
    max_items = int(sys.argv[2]) if len(sys.argv) > 2 else MAX_PRODUCTS
    run_crawler(keyword=keyword, max_products=max_items)
