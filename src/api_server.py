import sys
import os

# í˜„ì¬ íŒŒì¼ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬(src)ë¥¼ ë„˜ì–´, í”„ë¡œì íŠ¸ ë£¨íŠ¸(B2_crawling)ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from fastapi import FastAPI, Query
from opensearchpy import OpenSearch
from pydantic import BaseModel
from typing import List, Optional
import glob # ì¶”ê°€
import json # ì¶”ê°€
# api_server.py ìƒë‹¨ ì„í¬íŠ¸ ë¶€ë¶„ì— ì¶”ê°€
from fastapi.middleware.cors import CORSMiddleware
from fastapi import BackgroundTasks
from v4_safe_crawler import run_crawler as run_crawler_v4, stop_crawling as stop_crawling_v4, CRAWL_PROGRESS as CRAWL_PROGRESS_V4
from v5_fast_crawler import run_crawler as run_crawler_v5, stop_crawling as stop_crawling_v5, CRAWL_PROGRESS as CRAWL_PROGRESS_V5


# [Day 3] CRUD API ë¼ìš°í„° ì„í¬íŠ¸ (v4ì—ì„œëŠ” ë¯¸ì‚¬ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬)
# from routers import products_router

# 1. ì•±(App) ìƒì„±: ì„œë²„ì˜ ê°„íŒì„ ë‹µë‹ˆë‹¤.
app = FastAPI(
    title="Musinsa Search Engine",
    description="ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” APIì…ë‹ˆë‹¤.",
    version="2.0.0"  # ë²„ì „ ì—…!
)

# [Day 3] ìƒí’ˆ CRUD ë¼ìš°í„° ë“±ë¡ (v4ì—ì„œëŠ” ë¯¸ì‚¬ìš©ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬)
# app.include_router(products_router)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ê³³ì—ì„œ ì ‘ì† í—ˆìš© (ë³´ì•ˆìƒ ì‹¤ë¬´ì—ì„  íŠ¹ì • ë„ë©”ì¸ë§Œ ë„£ì§€ë§Œ, ì§€ê¸ˆì€ *ë¡œ)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. OpenSearch ì—°ê²°: ì°½ê³ ì§€ê¸° ê³ ìš©
client = OpenSearch(
    hosts=[{'host': 'localhost', 'port': 9201}],
    http_compress=True,
    use_ssl=False,
    verify_certs=False,
)

INDEX_NAME = "musinsa_products"

# 3. ì‘ë‹µ ëª¨ë¸ ì •ì˜ (Pydantic): ì†ë‹˜ì—ê²Œ ë³´ì—¬ì¤„ ë©”ë‰´íŒ ì–‘ì‹
# ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ì •ì˜í•´ë‘ë©´, FastAPIê°€ ì•Œì•„ì„œ ê²€ì‚¬í•˜ê³  ë¬¸ì„œë¥¼ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
class SellerInfo(BaseModel):
    company: Optional[str] = None
    brand: Optional[str] = None
    contact: Optional[str] = None
    email: Optional[str] = None
    biz_num: Optional[str] = None
    address: Optional[str] = None
    ceo: Optional[str] = None       # ì¶”ê°€
    license: Optional[str] = None   # ì¶”ê°€


class ProductSchema(BaseModel):
    goodsNo: Optional[int] = None   # ì¶”ê°€
    title: Optional[str] = None
    brand: Optional[str] = None
    price: Optional[int] = 0
    normalPrice: Optional[int] = 0  # ì¶”ê°€
    saleRate: Optional[int] = 0     # ì¶”ê°€
    url: Optional[str] = None
    thumbnail: Optional[str] = None # ì¶”ê°€
    seller_info: Optional[SellerInfo] = None

class SearchResponse(BaseModel):
    total: int
    items: List[ProductSchema]

# 4. ê²€ìƒ‰ API ë§Œë“¤ê¸° (GET /search)
# ì‚¬ìš©ìê°€ /search?keyword=íŒ¨ë”©&min_price=50000 ì²˜ëŸ¼ ìš”ì²­í•˜ë©´ ì´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.
# [Day 5~7] Redis ìºì‹± ì ìš©
from cache import get_cache, set_cache, generate_cache_key

@app.get("/search", response_model=SearchResponse, tags=["ê²€ìƒ‰"])
def search_products(
    keyword: str = Query(..., description="ê²€ìƒ‰í•  ìƒí’ˆëª… (ì˜ˆ: íŒ¨ë”©)"),
    min_price: int = Query(None, description="ìµœì†Œ ê°€ê²©"),
    max_price: int = Query(None, description="ìµœëŒ€ ê°€ê²©"),
    skip: int = Query(0, description="ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜ (í˜ì´ì§€ë„¤ì´ì…˜)")  # ì¶”ê°€
):
    # --- [1. ìºì‹œ í™•ì¸] ---
    # ìºì‹œ í‚¤ì— skip í¬í•¨ (í˜ì´ì§€ë³„ë¡œ ìºì‹±)
    cache_key = generate_cache_key("search", keyword=keyword, min_price=min_price, max_price=max_price, skip=skip)
    cached_result = get_cache(cache_key)
    
    if cached_result is not None:
        # ìºì‹œ íˆíŠ¸! OpenSearch ì¿¼ë¦¬ ì—†ì´ ë°”ë¡œ ë°˜í™˜
        return cached_result
    
    # --- [2. ìºì‹œ ë¯¸ìŠ¤ â†’ OpenSearch ê²€ìƒ‰] ---
    search_query = {
        "query": {
            "bool": {
                "must": [
                    {
                        "multi_match": {
                            "query": keyword,
                            "fields": ["title^2", "brand"],
                            "analyzer": "nori"
                        }
                    }
                ],
                "filter": []
            }
        },
        "from": skip,     # ì‹œì‘ ìœ„ì¹˜ ì ìš©
        "size": 20        # ë¬´í•œ ìŠ¤í¬ë¡¤ì„ ìœ„í•´ í•œ ë²ˆì— 20ê°œì”© ë¡œë”©
    }

    # ê°€ê²© í•„í„°
    if min_price or max_price:
        price_range = {"range": {"price": {}}}
        if min_price:
            price_range["range"]["price"]["gte"] = min_price
        if max_price:
            price_range["range"]["price"]["lte"] = max_price
        search_query["query"]["bool"]["filter"].append(price_range)

    try:
        response = client.search(body=search_query, index=INDEX_NAME)
    except Exception as e:
        print(f"Error: {e}")
        return {"total": 0, "items": []}

    total_hits = response["hits"]["total"]["value"]
    items = [hit["_source"] for hit in response["hits"]["hits"]]
    
    result_data = {"total": total_hits, "items": items}

    # --- [3. ê²°ê³¼ ìºì‹±] ---
    set_cache(cache_key, result_data, ttl=300)  # 5ë¶„ê°„ ìºì‹œ
    
    return result_data

# ğŸ†• ë²¡í„° ê²€ìƒ‰ API (k-NN ì‹œë§¨í‹± ê²€ìƒ‰)
from embedding_model import encode_text, get_model

@app.get("/search/vector", response_model=SearchResponse, tags=["ê²€ìƒ‰"])
def vector_search(
    keyword: str = Query(..., description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì‹œë§¨í‹± ê²€ìƒ‰)"),
    k: int = Query(20, description="ë°˜í™˜í•  ìƒí’ˆ ìˆ˜"),
    min_price: int = Query(None, description="ìµœì†Œ ê°€ê²©"),
    max_price: int = Query(None, description="ìµœëŒ€ ê°€ê²©")
):
    """
    ğŸš€ ë²¡í„° ê¸°ë°˜ ì‹œë§¨í‹± ê²€ìƒ‰
    - ê²€ìƒ‰ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬í•œ ìƒí’ˆ ê²€ìƒ‰
    - 'íŒ¨ë”©' ê²€ìƒ‰ ì‹œ 'ë‹¤ìš´ìì¼“', 'í‘¸í¼' ë“± ì—°ê´€ ìƒí’ˆë„ ê²€ìƒ‰ë¨
    """
    # ê²€ìƒ‰ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    query_vector = encode_text(keyword)
    
    # k-NN ê²€ìƒ‰ ì¿¼ë¦¬
    knn_query = {
        "size": k,
        "query": {
            "knn": {
                "title_vector": {
                    "vector": query_vector,
                    "k": k
                }
            }
        },
        "_source": {
            "excludes": ["title_vector"]  # ë²¡í„° í•„ë“œëŠ” ì‘ë‹µì—ì„œ ì œì™¸
        }
    }
    
    # ê°€ê²© í•„í„°ê°€ ìˆìœ¼ë©´ post_filterë¡œ ì ìš©
    if min_price or max_price:
        price_filter = {"range": {"price": {}}}
        if min_price:
            price_filter["range"]["price"]["gte"] = min_price
        if max_price:
            price_filter["range"]["price"]["lte"] = max_price
        knn_query["post_filter"] = {"bool": {"filter": [price_filter]}}
    
    try:
        response = client.search(body=knn_query, index=INDEX_NAME)
    except Exception as e:
        print(f"Vector Search Error: {e}")
        return {"total": 0, "items": []}
    
    total_hits = response["hits"]["total"]["value"]
    items = []
    for hit in response["hits"]["hits"]:
        item = hit["_source"]
        item["_score"] = hit["_score"]  # ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨
        items.append(item)
    
    return {"total": total_hits, "items": items}


# 5. í¬ë¡¤ë§ íŠ¸ë¦¬ê±° API (POST /crawl)
class CrawlRequest(BaseModel):
    keyword: str
    max_products: int = 100
    version: str = "v4"  # "v4" or "v5"

@app.post("/crawl", tags=["í¬ë¡¤ë§"])
def trigger_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    version: "v4" (ì•ˆì •) ë˜ëŠ” "v5" (ë¹ ë¦„)
    """
    if request.version == "v5":
        background_tasks.add_task(run_crawler_v5, keyword=request.keyword, max_products=request.max_products)
        version_label = "v5 (ë³‘ë ¬ ì²˜ë¦¬)"
    else:
        background_tasks.add_task(run_crawler_v4, keyword=request.keyword, max_products=request.max_products)
        version_label = "v4 (ì•ˆì •)"
    
    return {
        "message": f"í¬ë¡¤ë§ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´: {request.keyword}, ëª©í‘œ: {request.max_products}ê°œ, ë²„ì „: {version_label})",
        "status": "processing",
        "version": request.version
    }

@app.post("/stop-crawl", tags=["í¬ë¡¤ë§"])
def request_stop_crawling(version: str = "v4"):
    """
    ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ ì‘ì—…ì„ ì¦‰ì‹œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
    """
    if version == "v5":
        stop_crawling_v5()
    else:
        stop_crawling_v4()
    return {"message": "í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­ì„ ë³´ëƒˆìŠµë‹ˆë‹¤.", "status": "stopped", "version": version}

@app.get("/crawl-status", tags=["í¬ë¡¤ë§"])
def get_crawl_status(version: str = None):
    """
    í˜„ì¬ í¬ë¡¤ë§ ì§„í–‰ ìƒí™©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    versionì´ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ ì‹¤í–‰ëœ ë²„ì „ì˜ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if version == "v5":
        return CRAWL_PROGRESS_V5
    elif version == "v4":
        return CRAWL_PROGRESS_V4
    else:
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤í–‰ëœ ë²„ì „ ë°˜í™˜
        if CRAWL_PROGRESS_V5.get("start_time", 0) > CRAWL_PROGRESS_V4.get("start_time", 0):
            return CRAWL_PROGRESS_V5
        return CRAWL_PROGRESS_V4

@app.get("/latest-crawl", tags=["í¬ë¡¤ë§"])
def get_latest_crawl_result():
    """
    ê°€ì¥ ìµœê·¼ì— ìˆ˜ì§‘ëœ JSON íŒŒì¼ì˜ ë‚´ìš©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # data í´ë” ë‚´ì˜ crawl_result_*.json íŒ¨í„´ ê²€ìƒ‰
    list_of_files = glob.glob('data/crawl_result_*.json')
    
    if not list_of_files:
        return {"total": 0, "items": [], "message": "ì•„ì§ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
    # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ìµœì‹  íŒŒì¼ ì°¾ê¸°
    latest_file = max(list_of_files, key=os.path.getmtime)
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        return {
            "total": len(data), 
            "items": data, 
            "filename": os.path.basename(latest_file),
            "message": "ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        return {"total": 0, "items": [], "message": f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"}

# í„°ë¯¸ë„ì—ì„œ 'python api_server.py'ë¡œ ì‹¤í–‰í•  ë•Œë¥¼ ìœ„í•œ ì½”ë“œ
if __name__ == "__main__":
    import uvicorn
    # reload=True: ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ì„œë²„ê°€ ì•Œì•„ì„œ ì¬ì‹œì‘ë¨ (ê°œë°œí•  ë•Œ ê¿€ê¸°ëŠ¥)
    uvicorn.run("api_server:app", host="0.0.0.0", port=8000, reload=True)