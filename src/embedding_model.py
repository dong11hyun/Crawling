"""
ì„ë² ë”© ëª¨ë¸ ëª¨ë“ˆ
- í•œêµ­ì–´ ì§€ì› ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸
- ìƒí’ˆ ì œëª©ì„ 768ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
"""
from sentence_transformers import SentenceTransformer
import numpy as np
from functools import lru_cache

# ëª¨ë¸ ì´ë¦„ (í•œêµ­ì–´ ì§€ì› ë‹¤êµ­ì–´ ëª¨ë¸)
MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"

# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© (í•œ ë²ˆë§Œ ë¡œë“œ)
_model = None

def get_model() -> SentenceTransformer:
    """ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _model
    if _model is None:
        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")
        _model = SentenceTransformer(MODEL_NAME)
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì°¨ì›: {_model.get_sentence_embedding_dimension()})")
    return _model


def encode_text(text: str) -> list:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    
    Args:
        text: ë³€í™˜í•  í…ìŠ¤íŠ¸ (ì˜ˆ: ìƒí’ˆ ì œëª©)
    
    Returns:
        768ì°¨ì› ë²¡í„° (list)
    """
    model = get_model()
    embedding = model.encode(text, normalize_embeddings=True)
    return embedding.tolist()


def encode_texts_batch(texts: list, batch_size: int = 32, show_progress: bool = True) -> list:
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë³€í™˜ (ëŒ€ëŸ‰ ì²˜ë¦¬ìš©)
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
    
    Returns:
        ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    model = get_model()
    embeddings = model.encode(
        texts, 
        batch_size=batch_size, 
        normalize_embeddings=True,
        show_progress_bar=show_progress
    )
    return embeddings.tolist()


def get_embedding_dimension() -> int:
    """ì„ë² ë”© ë²¡í„° ì°¨ì› ë°˜í™˜"""
    return get_model().get_sentence_embedding_dimension()


# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    test_texts = [
        "íŒ¨ë”©",
        "ë”°ëœ»í•œ ê²¨ìš¸ ì•„ìš°í„°",
        "ë‹¤ìš´ìì¼“ í‘¸í¼",
        "ë‚¨ì„±ìš© ë¡±íŒ¨ë”©"
    ]
    
    print("\nğŸ§ª ì„ë² ë”© í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    embeddings = encode_texts_batch(test_texts, show_progress=False)
    
    for text, emb in zip(test_texts, embeddings):
        print(f"'{text}' â†’ ë²¡í„° ì°¨ì›: {len(emb)}")
    
    # ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    from numpy import dot
    from numpy.linalg import norm
    
    def cosine_sim(a, b):
        return dot(a, b) / (norm(a) * norm(b))
    
    print("\nğŸ“Š ì½”ì‚¬ì¸ ìœ ì‚¬ë„")
    print("-" * 50)
    base = embeddings[0]  # "íŒ¨ë”©"
    for text, emb in zip(test_texts, embeddings):
        sim = cosine_sim(base, emb)
        print(f"'íŒ¨ë”©' â†” '{text}': {sim:.4f}")
