"""
OpenSearch ì¬ì ì¬ ìŠ¤í¬ë¦½íŠ¸
- JSONL íŒŒì¼ì„ ì½ì–´ì„œ OpenSearchì— bulk ì ì¬
- ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë˜ëŠ” ìœ ì§€ ì˜µì…˜ ì œê³µ
python src/reload_opensearch.py "data/crawl_result_v5_íŒ¨ë”©_20260202_180355.json" --recreate
"""
import json
import os
import sys
import argparse
from datetime import datetime
from opensearchpy import OpenSearch, helpers

# ================= ì„¤ì • =================
HOST = 'localhost'
PORT = 9201
INDEX_NAME = "musinsa_products"

# ì¸ë±ìŠ¤ ë§¤í•‘ (init_opensearch.pyì™€ ë™ì¼)
INDEX_BODY = {
    "settings": {
        "index": {
            "analysis": {
                "tokenizer": {
                    "nori_user_dict": {
                        "type": "nori_tokenizer",
                        "decompound_mode": "mixed"
                    }
                },
                "analyzer": {
                    "korean_analyzer": {
                        "type": "custom",
                        "tokenizer": "nori_user_dict"
                    }
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "goodsNo": { "type": "integer" },
            "title": {
                "type": "text",
                "analyzer": "korean_analyzer",
                "fields": {
                    "keyword": { "type": "keyword" }
                }
            },
            "brand": { "type": "keyword" },
            "price": { "type": "integer" },
            "normalPrice": { "type": "integer" },
            "saleRate": { "type": "integer" },
            "url": { "type": "keyword" },
            "thumbnail": { "type": "keyword" },
            "crawled_at": { "type": "date" },
            "seller_info": {
                "type": "object",
                "properties": {
                    "company": { "type": "keyword" },
                    "ceo": { "type": "keyword" },
                    "biz_num": { "type": "keyword" },
                    "license": { "type": "keyword" },
                    "contact": { "type": "keyword" },
                    "email": { "type": "keyword" },
                    "address": { "type": "text" }
                }
            }
        }
    }
}


def get_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[{'host': HOST, 'port': PORT}],
        http_auth=('admin', 'admin'),
        use_ssl=False,
        verify_certs=False,
        timeout=60
    )


def create_index(client, index_name: str, recreate: bool = False):
    """ì¸ë±ìŠ¤ ìƒì„± (recreate=Trueë©´ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ìƒì„±)"""
    if recreate and client.indices.exists(index=index_name):
        client.indices.delete(index=index_name)
        print(f"ğŸ—‘ï¸  ê¸°ì¡´ '{index_name}' ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ")
    
    if not client.indices.exists(index=index_name):
        client.indices.create(index=index_name, body=INDEX_BODY)
        print(f"âœ… '{index_name}' ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    else:
        print(f"â„¹ï¸  '{index_name}' ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì ì¬í•©ë‹ˆë‹¤.")


def load_data(filepath: str) -> list:
    """JSON ë˜ëŠ” JSONL íŒŒì¼ ë¡œë“œ (í™•ì¥ìë¡œ ìë™ íŒë³„)"""
    items = []
    
    # í™•ì¥ì í™•ì¸
    is_jsonl = filepath.lower().endswith('.jsonl')
    
    with open(filepath, 'r', encoding='utf-8') as f:
        if is_jsonl:
            # JSONL: ì¤„ ë‹¨ìœ„ë¡œ íŒŒì‹±
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    item = json.loads(line)
                    items.append(item)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  {line_num}ë²ˆì§¸ ì¤„ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        else:
            # JSON: ì „ì²´ íŒŒì¼ì„ ë°°ì—´ë¡œ íŒŒì‹±
            try:
                items = json.load(f)
                if not isinstance(items, list):
                    print("âš ï¸  JSON íŒŒì¼ì´ ë°°ì—´ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
                    items = [items]
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                return []
    
    return items


def bulk_load(client, items: list, index_name: str, batch_size: int = 500):
    """Bulk APIë¡œ ë°ì´í„° ì ì¬"""
    total = len(items)
    success_count = 0
    fail_count = 0
    
    print(f"\nğŸ“¦ ì´ {total:,}ê°œ ì•„ì´í…œ ì ì¬ ì‹œì‘...")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print("-" * 50)
    
    for i in range(0, total, batch_size):
        batch = items[i:i+batch_size]
        actions = []
        
        for item in batch:
            doc_id = str(item.get('goodsNo', ''))
            if not doc_id:
                fail_count += 1
                continue
            
            # crawled_atì´ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
            if 'crawled_at' not in item:
                item['crawled_at'] = datetime.now().isoformat()
            
            actions.append({
                "_index": index_name,
                "_id": doc_id,
                "_source": item
            })
        
        try:
            success, failed = helpers.bulk(client, actions, refresh=False)
            success_count += success
            if failed:
                fail_count += len(failed)
            
            progress = min(i + batch_size, total)
            pct = (progress / total) * 100
            print(f"   [{progress:,}/{total:,}] ({pct:.1f}%) - ì„±ê³µ: {success_count:,}, ì‹¤íŒ¨: {fail_count}")
            
        except Exception as e:
            print(f"âŒ Bulk ì ì¬ ì˜¤ë¥˜: {e}")
            fail_count += len(batch)
    
    # ìµœì¢… refresh
    client.indices.refresh(index=index_name)
    
    return success_count, fail_count


def main():
    parser = argparse.ArgumentParser(
        description="JSON/JSONL íŒŒì¼ì„ OpenSearchì— ì¬ì ì¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python reload_opensearch.py data/crawl_result_v5_íŒ¨ë”©_20260202_180355.json
  python reload_opensearch.py data/crawl_progress_v5_íŒ¨ë”©.jsonl --recreate
  python reload_opensearch.py data/crawl_result.json --batch-size 1000
        """
    )
    parser.add_argument("data_file", help="ì ì¬í•  JSON ë˜ëŠ” JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--recreate", action="store_true", 
                        help="ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„± (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)")
    parser.add_argument("--index", default=INDEX_NAME, 
                        help=f"ì¸ë±ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: {INDEX_NAME})")
    parser.add_argument("--batch-size", type=int, default=500, 
                        help="Bulk ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 500)")
    
    args = parser.parse_args()
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.data_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_file}")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸš€ OpenSearch ì¬ì ì¬ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)
    print(f"   íŒŒì¼: {args.data_file}")
    print(f"   ì¸ë±ìŠ¤: {args.index}")
    print(f"   ì¸ë±ìŠ¤ ì´ˆê¸°í™”: {'ì˜ˆ' if args.recreate else 'ì•„ë‹ˆì˜¤ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)'}")
    print("=" * 60)
    
    # 1. OpenSearch ì—°ê²°
    print("\nğŸ”— OpenSearch ì—°ê²° ì¤‘...")
    client = get_client()
    
    if not client.ping():
        print("âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨!")
        sys.exit(1)
    print("   âœ… ì—°ê²° ì„±ê³µ")
    
    # 2. ì¸ë±ìŠ¤ ìƒì„±/í™•ì¸
    print("\nğŸ“‹ ì¸ë±ìŠ¤ ì„¤ì • ì¤‘...")
    create_index(client, args.index, args.recreate)
    
    # 3. JSON/JSONL íŒŒì¼ ë¡œë“œ
    print(f"\nğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    items = load_data(args.data_file)
    print(f"   âœ… {len(items):,}ê°œ ì•„ì´í…œ ë¡œë“œ ì™„ë£Œ")
    
    # 4. Bulk ì ì¬
    start_time = datetime.now()
    success, fail = bulk_load(client, items, args.index, args.batch_size)
    elapsed = (datetime.now() - start_time).total_seconds()
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ‰ ì ì¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ì„±ê³µ: {success:,}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"   ì²˜ë¦¬ ì†ë„: {success/elapsed:.1f}ê°œ/ì´ˆ")
    
    # ì¸ë±ìŠ¤ í†µê³„
    stats = client.indices.stats(index=args.index)
    doc_count = stats['indices'][args.index]['primaries']['docs']['count']
    print(f"\n   ğŸ“Š í˜„ì¬ ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {doc_count:,}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
