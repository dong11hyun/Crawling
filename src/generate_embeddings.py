"""
ìƒí’ˆ ë°ì´í„° ì„ë² ë”© ìƒì„± ë° OpenSearch ì ì¬ ìŠ¤í¬ë¦½íŠ¸
- JSON íŒŒì¼ì—ì„œ ìƒí’ˆ ë¡œë“œ
- ê° ìƒí’ˆ titleì„ ë²¡í„°ë¡œ ë³€í™˜
- k-NN ì¸ë±ìŠ¤ì— bulk ì ì¬
"""
import json
import os
import sys
import argparse
from datetime import datetime
from opensearchpy import OpenSearch, helpers

# ì„ë² ë”© ëª¨ë¸ ì„í¬íŠ¸
from embedding_model import encode_texts_batch, get_embedding_dimension

# ================= ì„¤ì • =================
HOST = 'localhost'
PORT = 9201
INDEX_NAME = "musinsa_products"


def get_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[{'host': HOST, 'port': PORT}],
        http_auth=('admin', 'admin'),
        use_ssl=False,
        verify_certs=False,
        timeout=60
    )


def load_data(filepath: str) -> list:
    """JSON ë˜ëŠ” JSONL íŒŒì¼ ë¡œë“œ"""
    is_jsonl = filepath.lower().endswith('.jsonl')
    
    with open(filepath, 'r', encoding='utf-8') as f:
        if is_jsonl:
            items = [json.loads(line) for line in f if line.strip()]
        else:
            items = json.load(f)
    
    return items


def generate_embeddings_and_load(client, items: list, batch_size: int = 100):
    """ì„ë² ë”© ìƒì„± í›„ OpenSearch ì ì¬"""
    total = len(items)
    success_count = 0
    fail_count = 0
    
    print(f"\nğŸ“¦ ì´ {total:,}ê°œ ì•„ì´í…œ ì„ë² ë”© ìƒì„± ë° ì ì¬ ì‹œì‘...")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   ë²¡í„° ì°¨ì›: {get_embedding_dimension()}")
    print("-" * 50)
    
    for i in range(0, total, batch_size):
        batch = items[i:i+batch_size]
        
        # 1. ì œëª© ì¶”ì¶œ
        titles = [item.get('title', '') for item in batch]
        
        # 2. ì„ë² ë”© ìƒì„± (ë°°ì¹˜)
        embeddings = encode_texts_batch(titles, show_progress=False)
        
        # 3. OpenSearch ì•¡ì…˜ ìƒì„±
        actions = []
        for item, embedding in zip(batch, embeddings):
            doc_id = str(item.get('goodsNo', ''))
            if not doc_id:
                fail_count += 1
                continue
            
            # ë²¡í„° í•„ë“œ ì¶”ê°€
            item['title_vector'] = embedding
            
            if 'crawled_at' not in item:
                item['crawled_at'] = datetime.now().isoformat()
            
            actions.append({
                "_index": INDEX_NAME,
                "_id": doc_id,
                "_source": item
            })
        
        # 4. Bulk ì ì¬
        try:
            success, failed = helpers.bulk(client, actions, refresh=False)
            success_count += success
            if failed:
                fail_count += len(failed)
        except Exception as e:
            print(f"âŒ Bulk ì˜¤ë¥˜: {e}")
            fail_count += len(batch)
        
        # ì§„í–‰ë¥  ì¶œë ¥
        progress = min(i + batch_size, total)
        pct = (progress / total) * 100
        print(f"   [{progress:,}/{total:,}] ({pct:.1f}%) - ì„±ê³µ: {success_count:,}")
    
    # ìµœì¢… refresh
    client.indices.refresh(index=INDEX_NAME)
    
    return success_count, fail_count


def main():
    parser = argparse.ArgumentParser(description="ìƒí’ˆ ë°ì´í„° ì„ë² ë”© ìƒì„± ë° OpenSearch ì ì¬")
    parser.add_argument("data_file", help="ì ì¬í•  JSON/JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--batch-size", type=int, default=100, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.data_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_file}")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸš€ ìƒí’ˆ ë°ì´í„° ì„ë² ë”© & OpenSearch ì ì¬")
    print("=" * 60)
    print(f"   íŒŒì¼: {args.data_file}")
    print(f"   ì¸ë±ìŠ¤: {INDEX_NAME}")
    print("=" * 60)
    
    # 1. OpenSearch ì—°ê²°
    print("\nğŸ”— OpenSearch ì—°ê²° ì¤‘...")
    client = get_client()
    
    if not client.ping():
        print("âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨!")
        sys.exit(1)
    print("   âœ… ì—°ê²° ì„±ê³µ")
    
    # 2. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘...")
    items = load_data(args.data_file)
    print(f"   âœ… {len(items):,}ê°œ ì•„ì´í…œ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ì„ë² ë”© ìƒì„± ë° ì ì¬
    start_time = datetime.now()
    success, fail = generate_embeddings_and_load(client, items, args.batch_size)
    elapsed = (datetime.now() - start_time).total_seconds()
    
    # 4. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ‰ ì„ë² ë”© & ì ì¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ì„±ê³µ: {success:,}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"   ì²˜ë¦¬ ì†ë„: {success/elapsed:.1f}ê°œ/ì´ˆ")
    
    # ì¸ë±ìŠ¤ í†µê³„
    stats = client.indices.stats(index=INDEX_NAME)
    doc_count = stats['indices'][INDEX_NAME]['primaries']['docs']['count']
    print(f"\n   ğŸ“Š í˜„ì¬ ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {doc_count:,}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    main()
