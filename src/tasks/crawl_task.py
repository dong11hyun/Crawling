"""
í¬ë¡¤ë§ Task ëª¨ë“ˆ
- ë¬´ì‹ ì‚¬ ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘
- v2.2_crawler.py ë¡œì§ ì¬ì‚¬ìš©
"""
import asyncio
import re
import os
import logging
from datetime import datetime
from typing import List, Dict, Any

from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


async def _extract_seller_info(page) -> Dict[str, str]:
    """
    íŒë§¤ì ì •ë³´ ì¶”ì¶œ (v2.2_crawler.py ë¡œì§ ì¬ì‚¬ìš©)
    """
    try:
        # íŒë§¤ì ì •ë³´ ë²„íŠ¼ í´ë¦­
        seller_btn = page.locator("button", has_text="íŒë§¤ì ì •ë³´")
        if await seller_btn.count() > 0:
            expanded = await seller_btn.get_attribute("aria-expanded")
            if expanded != "true":
                await seller_btn.click()
                await asyncio.sleep(0.5)
    except Exception as e:
        logger.warning(f"íŒë§¤ì ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
    
    async def get_value(label: str) -> str:
        """XPathë¡œ ë¼ë²¨ ì˜† í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        locator = page.locator(f"//dt[contains(., '{label}')]/following-sibling::dd[1]")
        if await locator.count() > 0:
            return await locator.inner_text()
        return ""
    
    return {
        "company": await get_value("ìƒí˜¸"),
        "brand": await get_value("ë¸Œëœë“œ"),
        "biz_num": await get_value("ì‚¬ì—…ìë²ˆí˜¸"),
        "license": await get_value("í†µì‹ íŒë§¤ì—…ì‹ ê³ "),
        "contact": await get_value("ì—°ë½ì²˜"),
        "email": await get_value("E-mail"),
        "address": await get_value("ì˜ì—…ì†Œì¬ì§€"),
    }


async def _crawl_async(keyword: str, scroll_count: int, max_products: int) -> List[Dict[str, Any]]:
    """
    ë¹„ë™ê¸° í¬ë¡¤ë§ ë©”ì¸ ë¡œì§
    """
    target_url = f"https://www.musinsa.com/search/goods?keyword={keyword}&gf=A"
    collected_data = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()
        
        logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
        await page.goto(target_url, timeout=60000)
        
        # ìŠ¤í¬ë¡¤
        for i in range(scroll_count):
            await page.keyboard.press("End")
            await asyncio.sleep(2)
            logger.info(f"   â¬‡ï¸ ìŠ¤í¬ë¡¤ {i+1}/{scroll_count}")
        
        # ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
        locators = page.locator("a[href*='/products/']")
        count = await locators.count()
        urls = set()
        for i in range(min(count, max_products)):
            href = await locators.nth(i).get_attribute("href")
            if href:
                if not href.startswith("http"):
                    href = "https://www.musinsa.com" + href
                urls.add(href)
        
        url_list = list(urls)
        logger.info(f"   ğŸ“¦ ìˆ˜ì§‘ ëŒ€ìƒ: {len(url_list)}ê°œ")
        
        for idx, url in enumerate(url_list):
            try:
                logger.info(f"   [{idx+1}/{len(url_list)}] {url}")
                await page.goto(url, timeout=30000)
                await page.wait_for_load_state("domcontentloaded")
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_loc = page.locator("meta[property='og:title']").first
                title = await title_loc.get_attribute("content") if await title_loc.count() > 0 else "ì œëª©ì—†ìŒ"
                
                brand_loc = page.locator("meta[property='product:brand']").first
                brand = await brand_loc.get_attribute("content") if await brand_loc.count() > 0 else ""
                
                price_loc = page.locator("meta[property='product:price:amount']").first
                if await price_loc.count() > 0:
                    price_text = await price_loc.get_attribute("content")
                    price_num = re.sub(r'[^0-9]', '', str(price_text))
                    price = int(price_num) if price_num else 0
                else:
                    price = 0
                
                # íŒë§¤ì ì •ë³´ ì¶”ì¶œ (ê°œì„ !)
                seller_info = await _extract_seller_info(page)
                
                collected_data.append({
                    "url": url,
                    "title": title,
                    "brand": brand or seller_info.get("brand", ""),
                    "price": price,
                    "seller_info": seller_info,
                    "crawled_at": datetime.now().isoformat()
                })
                
                logger.info(f"      âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:30]}...")
                
            except Exception as e:
                logger.error(f"      âŒ ì—ëŸ¬: {e}")
                continue
        
        await browser.close()
    
    return collected_data


def crawl_musinsa(
    keyword: str = "íŒ¨ë”©",
    scroll_count: int = 2,
    max_products: int = 10
) -> List[Dict[str, Any]]:
    """
    ë¬´ì‹ ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰ (ë™ê¸° ë˜í¼)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        scroll_count: ìŠ¤í¬ë¡¤ íšŸìˆ˜
        max_products: ìµœëŒ€ ìˆ˜ì§‘ ìƒí’ˆ ìˆ˜
    
    Returns:
        ìˆ˜ì§‘ëœ ìƒí’ˆ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸ“¦ í¬ë¡¤ë§ íŒŒë¼ë¯¸í„°: keyword={keyword}, scroll={scroll_count}, max={max_products}")
    
    data = asyncio.run(_crawl_async(keyword, scroll_count, max_products))
    
    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(data)}ê±´")
    return data
