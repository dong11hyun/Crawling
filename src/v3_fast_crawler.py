"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v3 - í•˜ì´ë¸Œë¦¬ë“œ (API + HTML íŒŒì‹±)
- 1ë‹¨ê³„: ìƒí’ˆ ëª©ë¡ APIë¡œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘
- 2ë‹¨ê³„: ìƒí’ˆ ìƒì„¸ HTML íŒŒì‹±ìœ¼ë¡œ íŒë§¤ìž ì •ë³´ ìˆ˜ì§‘

ì†ë„: Playwright ëŒ€ë¹„ ì•½ 10ë°° ë¹ ë¦„
"""
import requests
from bs4 import BeautifulSoup
import time
import json

# ================= ì„¤ì • =================
SEARCH_KEYWORD = "íŒ¨ë”©"
MAX_PRODUCTS = 10

# í—¤ë” ì„¤ì • (User-Agent í•„ìˆ˜)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Referer": "https://www.musinsa.com/"
}


def get_product_list(keyword: str, size: int = 10) -> list:
    """
    1ë‹¨ê³„: ìƒí’ˆ ê²€ìƒ‰ APIë¡œ ìƒí’ˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    """
    url = "https://api.musinsa.com/api2/dp/v1/plp/goods"
    params = {
        "gf": "A",
        "keyword": keyword,
        "sortCode": "POPULAR",
        "isUsed": "false",
        "page": 1,
        "size": size,
        "caller": "SEARCH"
    }
    
    print(f"ðŸ” [1ë‹¨ê³„] '{keyword}' ê²€ìƒ‰ API í˜¸ì¶œ ì¤‘...")
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        products = data.get("data", {}).get("list", [])
        print(f"   âœ… ìƒí’ˆ {len(products)}ê°œ ë°œê²¬!")
        
        return products
    except Exception as e:
        print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def get_seller_info(goods_no: int) -> dict:
    """
    2ë‹¨ê³„: ìƒí’ˆ ìƒì„¸ íŽ˜ì´ì§€ì˜ __NEXT_DATA__ JSONì—ì„œ íŒë§¤ìž ì •ë³´ ì¶”ì¶œ
    """
    url = f"https://www.musinsa.com/products/{goods_no}"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # __NEXT_DATA__ script íƒœê·¸ì—ì„œ JSON ì¶”ì¶œ
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        if not next_data_script:
            print(f"      âš ï¸ __NEXT_DATA__ not found")
            return {}
        
        import json
        data = json.loads(next_data_script.string)
        
        # company ê°ì²´ ì¶”ì¶œ
        company = data.get('props', {}).get('pageProps', {}).get('meta', {}).get('data', {}).get('company', {})
        
        if not company:
            return {}
        
        seller_info = {
            "company": company.get('name', ''),
            "ceo": company.get('ceoName', ''),
            "biz_num": company.get('businessNumber', ''),
            "license": company.get('mailOrderReportNumber', ''),
            "contact": company.get('phoneNumber', ''),
            "email": company.get('email', ''),
            "address": f"{company.get('address', '')} {company.get('detailAddress', '')}".strip()
        }
        
        return seller_info
    
    except Exception as e:
        print(f"      âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {}


def run_crawler(keyword: str = SEARCH_KEYWORD, max_products: int = MAX_PRODUCTS):
    """
    ë©”ì¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    """
    start_time = time.time()
    
    print("=" * 50)
    print("ðŸš€ ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v3 (í•˜ì´ë¸Œë¦¬ë“œ) ì‹œìž‘")
    print("=" * 50)
    
    # 1ë‹¨ê³„: ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ
    products = get_product_list(keyword, max_products)
    
    if not products:
        print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # 2ë‹¨ê³„: ê° ìƒí’ˆì˜ íŒë§¤ìž ì •ë³´ ìˆ˜ì§‘
    print(f"\nðŸ“¦ [2ë‹¨ê³„] íŒë§¤ìž ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    
    results = []
    for idx, product in enumerate(products):
        goods_no = product.get("goodsNo")
        goods_name = product.get("goodsName", "")[:30]  # 30ìž ì œí•œ
        
        print(f"   [{idx+1}/{len(products)}] {goods_name}...")
        
        # íŒë§¤ìž ì •ë³´ íŒŒì‹±
        seller_info = get_seller_info(goods_no)
        
        # ê²°ê³¼ ì¡°í•©
        result = {
            "goodsNo": goods_no,
            "title": product.get("goodsName"),
            "brand": product.get("brandName"),
            "price": product.get("price"),
            "normalPrice": product.get("normalPrice"),
            "saleRate": product.get("saleRate"),
            "url": f"https://www.musinsa.com/products/{goods_no}",
            "thumbnail": product.get("thumbnail"),
            "seller_info": seller_info
        }
        results.append(result)
        
        # Rate limiting (0.5ì´ˆ ë”œë ˆì´)
        time.sleep(0.5)
    
    # ê²°ê³¼ ì¶œë ¥
    elapsed = time.time() - start_time
    print("\n" + "=" * 50)
    print(f"ðŸŽ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(results)}ê°œ ìƒí’ˆ")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print("=" * 50)
    
    # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
    print("\nðŸ“‹ ìˆ˜ì§‘ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
    for r in results[:3]:
        print(f"\n   ðŸ“¦ {r['title'][:40]}...")
        print(f"      ê°€ê²©: {r['price']:,}ì›")
        print(f"      íŒë§¤ìž: {r['seller_info'].get('company', 'N/A')}")
        print(f"      ì—°ë½ì²˜: {r['seller_info'].get('contact', 'N/A')}")
    
    # JSON íŒŒì¼ë¡œ ì €ìž¥
    output_file = f"data/crawl_result_{keyword}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥: {output_file}")
    
    return results


if __name__ == "__main__":
    import sys
    keyword = sys.argv[1] if len(sys.argv) > 1 else SEARCH_KEYWORD
    run_crawler(keyword=keyword)
