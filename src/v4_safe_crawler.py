"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v4 - ì•ˆì „ ìˆ˜ì§‘ (1000ê±´)
- Rate Limiting + ëœë¤ ë”œë ˆì´
- User-Agent ë¡œí…Œì´ì…˜
- 429 ì—ëŸ¬ í•¸ë“¤ë§
- ì§„í–‰ ìƒíƒœ ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ)
"""
import requests
from bs4 import BeautifulSoup
import time
import json
import random
import os
from datetime import datetime
from opensearchpy import OpenSearch  # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€

# ================= ì„¤ì • =================
SEARCH_KEYWORD = "íŒ¨ë”©"
MAX_PRODUCTS = 1000
BATCH_SIZE = 60  # APIì—ì„œ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜

# ë”œë ˆì´ ì„¤ì • (ì´ˆ) - 1.8ë°° ì¦ì†
API_DELAY = 0.6      # ëª©ë¡ API
HTML_DELAY = 1.2     # ìƒì„¸ í˜ì´ì§€
RANDOM_RANGE = (0.3, 0.7)  # ì¶”ê°€ ëœë¤ ë”œë ˆì´

# User-Agent ë¡œí…Œì´ì…˜
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

# íŒŒì¼ ê²½ë¡œ
PROGRESS_FILE = "data/progress.json"
JSONL_FILE = "data/crawl_progress_{keyword}.jsonl"  # ì ì§„ì  ì €ì¥ìš©
OUTPUT_FILE = "data/crawl_result_{keyword}_{timestamp}.json"

# ì „ì—­ ì¤‘ì§€ í”Œë˜ê·¸
STOP_CRAWLER_FLAG = False

def stop_crawling():
    """í¬ë¡¤ë§ ì¤‘ì§€"""
    global STOP_CRAWLER_FLAG
    STOP_CRAWLER_FLAG = True
    print("\nğŸ›‘ í¬ë¡¤ë§ ì¤‘ì§€ ìš”ì²­ë¨!")

# ì „ì—­ ì§„í–‰ ìƒíƒœ (ì´ˆê¸°ê°’)
CRAWL_PROGRESS = {
    "status": "idle",       # idle, running, finished, stopped
    "keyword": "",
    "total": 0,             # ëª©í‘œ ê°œìˆ˜
    "current": 0,           # í˜„ì¬ ìˆ˜ì§‘ ê°œìˆ˜
    "start_time": 0.0,      # ì‹œì‘ ì‹œê°„ (ETA ê³„ì‚°ìš©)
}



def get_headers():
    """ëœë¤ User-Agent í—¤ë” ìƒì„±"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "application/json, text/html",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Referer": "https://www.musinsa.com/",
    }


def safe_delay(base_delay: float):
    """ëœë¤ ë”œë ˆì´ ì ìš©"""
    delay = base_delay + random.uniform(*RANDOM_RANGE)
    time.sleep(delay)


def load_progress() -> set:
    """ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            return set(data.get('collected_ids', []))
    return set()


def save_progress(collected_ids: set):
    """ì§„í–‰ ìƒíƒœ ì €ì¥"""
    os.makedirs('data', exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({'collected_ids': list(collected_ids)}, f)


def append_to_jsonl(filepath: str, data: dict):
    """JSONL íŒŒì¼ì— í•œ ì¤„ì”© ì¶”ê°€ (ì ì§„ì  ì €ì¥)"""
    os.makedirs('data', exist_ok=True)
    with open(filepath, 'a', encoding='utf-8') as f:
        f.write(json.dumps(data, ensure_ascii=False) + '\n')


def load_jsonl(filepath: str) -> list:
    """JSONL íŒŒì¼ì—ì„œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
    results = []
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    results.append(json.loads(line))
    return results


def get_product_list(keyword: str, page: int, size: int = 60, session=None) -> list:
    """ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ"""
    url = "https://api.musinsa.com/api2/dp/v1/plp/goods"
    params = {
        "gf": "A",
        "keyword": keyword,
        "sortCode": "POPULAR",
        "isUsed": "false",
        "page": page,
        "size": size,
        "caller": "SEARCH"
    }
    
    for attempt in range(3):
        try:
            response = session.get(url, params=params, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                print(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
            
            response.raise_for_status()
            data = response.json()
            return data.get("data", {}).get("list", [])
            
        except Exception as e:
            print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}")
            time.sleep(10)
    
    return []


def get_seller_info(goods_no: int, session=None) -> dict:
    """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ íŒë§¤ì ì •ë³´ ì¶”ì¶œ"""
    url = f"https://www.musinsa.com/products/{goods_no}"
    
    for attempt in range(3):
        try:
            response = session.get(url, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                print(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
                
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            next_data_script = soup.find('script', id='__NEXT_DATA__')
            
            if not next_data_script:
                return {}
            
            data = json.loads(next_data_script.string)
            company = data.get('props', {}).get('pageProps', {}).get('meta', {}).get('data', {}).get('company', {})
            
            if not company:
                return {}
            
            return {
                "company": company.get('name', ''),
                "ceo": company.get('ceoName', ''),
                "biz_num": company.get('businessNumber', ''),
                "license": company.get('mailOrderReportNumber', ''),
                "contact": company.get('phoneNumber', ''),
                "email": company.get('email', ''),
                "address": f"{company.get('address', '')} {company.get('detailAddress', '')}".strip()
            }
            
        except Exception as e:
            print(f"   âŒ HTML íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}")
            time.sleep(5)
    
    return {}


    return {}


def get_opensearch_client():
    """OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OpenSearch(
        hosts=[{'host': 'localhost', 'port': 9201}],  # Docker ë§¤í•‘ í¬íŠ¸ í™•ì¸
        http_auth=None,
        use_ssl=False,
        verify_certs=False,
        timeout=30
    )


def index_to_opensearch(client, data: dict, index_name: str = "musinsa_products"):
    """OpenSearchì— ë°ì´í„° ì‹¤ì‹œê°„ ì ì¬"""
    try:
        # IDë¥¼ ì§€ì •í•˜ì—¬ ì¤‘ë³µ ë°©ì§€ (Upsert íš¨ê³¼)
        doc_id = str(data['goodsNo'])
        
        # _index, _source êµ¬ì¡° ì œê±°í•˜ê³  ìˆœìˆ˜ ë°ì´í„°ë§Œ ì ì¬ (OpenSearchpyê°€ ì•Œì•„ì„œ ì²˜ë¦¬)
        # ë‹¨, ë‚ ì§œ í•„ë“œëŠ” ISO í˜•ì‹ìœ¼ë¡œ ë§ì¶”ëŠ”ê²Œ ì¢‹ìŒ
        if 'crawled_at' not in data:
            data['crawled_at'] = datetime.now().isoformat()

        client.index(
            index=index_name,
            body=data,
            id=doc_id,
            refresh=True # ì¦‰ì‹œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ í•¨ (ë¶€í•˜ê°€ ì¡°ê¸ˆ ìˆì§€ë§Œ ì‹¤ì‹œê°„ì„± ìœ„í•´)
        )
        # print(f"      ğŸš€ OpenSearch ì ì¬ ì™„ë£Œ: {doc_id}")
    except Exception as e:
        print(f"      âŒ OpenSearch ì ì¬ ì‹¤íŒ¨: {e}")


def run_crawler(keyword: str = SEARCH_KEYWORD, max_products: int = MAX_PRODUCTS):
    """ë©”ì¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    global STOP_CRAWLER_FLAG, CRAWL_PROGRESS
    STOP_CRAWLER_FLAG = False  # ì‹œì‘ ì‹œ í”Œë˜ê·¸ ì´ˆê¸°í™”
    
    start_time = time.time()
    
    # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
    CRAWL_PROGRESS.update({
        "status": "running",
        "keyword": keyword,
        "total": max_products,
        "current": 0,
        "start_time": start_time
    })
    
    print("=" * 60)
    print(f"ğŸš€ ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v4 (ì•ˆì „ ìˆ˜ì§‘) ì‹œì‘")
    print(f"   ê²€ìƒ‰ì–´: {keyword}")
    print(f"   ëª©í‘œ: {max_products}ê°œ")
    print("=" * 60)
    
    # ì„¸ì…˜ ìƒì„± (ì¿ í‚¤ ìœ ì§€)
    session = requests.Session()
    
    # OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    try:
        os_client = get_opensearch_client()
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if os_client.ping():
            print("   âœ… OpenSearch ì—°ê²° ì„±ê³µ")
        else:
            print("   âš ï¸ OpenSearch ì—°ê²° ì‹¤íŒ¨ (Docker í™•ì¸ í•„ìš”)")
            os_client = None
    except Exception as e:
        print(f"   âš ï¸ OpenSearch ì´ˆê¸°í™” ì—ëŸ¬: {e}")
        os_client = None
    
    # ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ
    collected_ids = load_progress()
    if collected_ids:
        print(f"ğŸ“‚ ì´ì „ ì§„í–‰ ìƒíƒœ ë³µì›: {len(collected_ids)}ê°œ ì´ë¯¸ ìˆ˜ì§‘ë¨")
    
    # 1ë‹¨ê³„: ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘
    print(f"\nğŸ” [1ë‹¨ê³„] ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ ì¤‘...")
    
    all_products = []
    page = 1
    
    while len(all_products) < max_products:
        if STOP_CRAWLER_FLAG:
            print("   ğŸ›‘ [1ë‹¨ê³„] ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œ")
            CRAWL_PROGRESS["status"] = "stopped"
            break

        products = get_product_list(keyword, page, BATCH_SIZE, session)
        
        if not products:
            print(f"   ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # ì´ë¯¸ ìˆ˜ì§‘í•œ í•­ëª© ì œì™¸
        new_products = [p for p in products if p.get('goodsNo') not in collected_ids]
        all_products.extend(new_products)
        
        print(f"   í˜ì´ì§€ {page}: {len(new_products)}ê°œ ì¶”ê°€ (ì´ {len(all_products)}ê°œ)")
        
        page += 1
        safe_delay(API_DELAY)
        
        if len(all_products) >= max_products:
            all_products = all_products[:max_products]
            break
    
    print(f"   âœ… ìˆ˜ì§‘í•  ìƒí’ˆ: {len(all_products)}ê°œ")
    
    # 2ë‹¨ê³„: íŒë§¤ì ì •ë³´ ìˆ˜ì§‘
    print(f"\nğŸ“¦ [2ë‹¨ê³„] íŒë§¤ì ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    print(f"   ğŸ’¾ ì ì§„ì  ì €ì¥ í™œì„±í™” (JSONL)")
    
    # JSONL íŒŒì¼ ê²½ë¡œ
    jsonl_path = JSONL_FILE.format(keyword=keyword)
    
    # ê¸°ì¡´ JSONL ë°ì´í„° ë¡œë“œ (ì¬ê°œ ì‹œ)
    results = load_jsonl(jsonl_path)
    if results:
        print(f"   ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë³µì›: {len(results)}ê°œ")
    
    total = len(all_products)
    
    for idx, product in enumerate(all_products):
        if STOP_CRAWLER_FLAG:
            print(f"   ğŸ›‘ [2ë‹¨ê³„] ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œ ({idx}ê°œ ìˆ˜ì§‘ë¨)")
            CRAWL_PROGRESS["status"] = "stopped"
            break

        goods_no = product.get("goodsNo")
        goods_name = product.get("goodsName", "")[:30]
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress = (idx + 1) / total * 100
        print(f"   [{idx+1}/{total}] ({progress:.1f}%) {goods_name}...")
        
        # íŒë§¤ì ì •ë³´ ì¶”ì¶œ
        seller_info = get_seller_info(goods_no, session)
        
        # ê²°ê³¼ ì¡°í•©
        result = {
            "goodsNo": goods_no,
            "title": product.get("goodsName"),
            "brand": product.get("brandName"),
            "price": product.get("price"),
            "normalPrice": product.get("normalPrice"),
            "saleRate": product.get("saleRate"),
            "url": f"https://www.musinsa.com/products/{goods_no}",
            "thumbnail": product.get("thumbnail"),
            "seller_info": seller_info
        }
        
        # âœ… ì¦‰ì‹œ JSONLì— ì €ì¥ (ì ì§„ì  ì €ì¥)
        append_to_jsonl(jsonl_path, result)
        results.append(result)
        
        # ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        CRAWL_PROGRESS["current"] = len(results)
        
        # âœ… OpenSearch ì‹¤ì‹œê°„ ì ì¬
        if os_client:
            index_to_opensearch(os_client, result)
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥ (100ê°œë§ˆë‹¤)
        collected_ids.add(goods_no)
        if (idx + 1) % 100 == 0:
            save_progress(collected_ids)
            print(f"   ğŸ’¾ ì§„í–‰ ìƒíƒœ ì €ì¥ ì™„ë£Œ ({idx+1}ê°œ)")
            print(f"   ğŸ“„ JSONL íŒŒì¼: {jsonl_path}")
        
        # ë”œë ˆì´
        safe_delay(HTML_DELAY)
    
    # ìµœì¢… ì €ì¥ (JSONL â†’ JSON ë³€í™˜)
    elapsed = time.time() - start_time
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_FILE.format(keyword=keyword, timestamp=timestamp)
    
    os.makedirs('data', exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ì§„í–‰ ìƒíƒœ ë° JSONL ì •ë¦¬
    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)
    if os.path.exists(jsonl_path):
        os.remove(jsonl_path)  # JSONLì€ JSONìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ í›„ ì‚­ì œ
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"   ì´ ìˆ˜ì§‘: {len(results)}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("=" * 60)
    
    # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    if not STOP_CRAWLER_FLAG:
        CRAWL_PROGRESS["status"] = "finished"
        CRAWL_PROGRESS["current"] = len(results)

    return results


if __name__ == "__main__":
    import sys
    keyword = sys.argv[1] if len(sys.argv) > 1 else SEARCH_KEYWORD
    max_items = int(sys.argv[2]) if len(sys.argv) > 2 else MAX_PRODUCTS
    run_crawler(keyword=keyword, max_products=max_items)
