"""
ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v4 - ì•ˆì „ ìˆ˜ì§‘ (1000ê±´)
- Rate Limiting + ëœë¤ ë”œë ˆì´
- User-Agent ë¡œí…Œì´ì…˜
- 429 ì—ëŸ¬ í•¸ë“¤ë§
- ì§„í–‰ ìƒíƒœ ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ)
"""
import requests
from bs4 import BeautifulSoup
import time
import json
import random
import os
from datetime import datetime

# ================= ì„¤ì • =================
SEARCH_KEYWORD = "íŒ¨ë”©"
MAX_PRODUCTS = 1000
BATCH_SIZE = 60  # APIì—ì„œ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜

# ë”œë ˆì´ ì„¤ì • (ì´ˆ)
API_DELAY = 1.0      # ëª©ë¡ API
HTML_DELAY = 2.0     # ìƒì„¸ í˜ì´ì§€
RANDOM_RANGE = (0.5, 1.5)  # ì¶”ê°€ ëœë¤ ë”œë ˆì´

# User-Agent ë¡œí…Œì´ì…˜
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

# íŒŒì¼ ê²½ë¡œ
PROGRESS_FILE = "data/progress.json"
OUTPUT_FILE = "data/crawl_result_{keyword}_{timestamp}.json"


def get_headers():
    """ëœë¤ User-Agent í—¤ë” ìƒì„±"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "application/json, text/html",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Referer": "https://www.musinsa.com/",
    }


def safe_delay(base_delay: float):
    """ëœë¤ ë”œë ˆì´ ì ìš©"""
    delay = base_delay + random.uniform(*RANDOM_RANGE)
    time.sleep(delay)


def load_progress() -> set:
    """ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            return set(data.get('collected_ids', []))
    return set()


def save_progress(collected_ids: set):
    """ì§„í–‰ ìƒíƒœ ì €ì¥"""
    os.makedirs('data', exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({'collected_ids': list(collected_ids)}, f)


def get_product_list(keyword: str, page: int, size: int = 60, session=None) -> list:
    """ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ"""
    url = "https://api.musinsa.com/api2/dp/v1/plp/goods"
    params = {
        "gf": "A",
        "keyword": keyword,
        "sortCode": "POPULAR",
        "isUsed": "false",
        "page": page,
        "size": size,
        "caller": "SEARCH"
    }
    
    for attempt in range(3):
        try:
            response = session.get(url, params=params, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                print(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
            
            response.raise_for_status()
            data = response.json()
            return data.get("data", {}).get("list", [])
            
        except Exception as e:
            print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}")
            time.sleep(10)
    
    return []


def get_seller_info(goods_no: int, session=None) -> dict:
    """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ íŒë§¤ì ì •ë³´ ì¶”ì¶œ"""
    url = f"https://www.musinsa.com/products/{goods_no}"
    
    for attempt in range(3):
        try:
            response = session.get(url, headers=get_headers(), timeout=15)
            
            if response.status_code == 429:
                print(f"   âš ï¸ 429 Too Many Requests - 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
                
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            next_data_script = soup.find('script', id='__NEXT_DATA__')
            
            if not next_data_script:
                return {}
            
            data = json.loads(next_data_script.string)
            company = data.get('props', {}).get('pageProps', {}).get('meta', {}).get('data', {}).get('company', {})
            
            if not company:
                return {}
            
            return {
                "company": company.get('name', ''),
                "ceo": company.get('ceoName', ''),
                "biz_num": company.get('businessNumber', ''),
                "license": company.get('mailOrderReportNumber', ''),
                "contact": company.get('phoneNumber', ''),
                "email": company.get('email', ''),
                "address": f"{company.get('address', '')} {company.get('detailAddress', '')}".strip()
            }
            
        except Exception as e:
            print(f"   âŒ HTML íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {e}")
            time.sleep(5)
    
    return {}


def run_crawler(keyword: str = SEARCH_KEYWORD, max_products: int = MAX_PRODUCTS):
    """ë©”ì¸ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    start_time = time.time()
    
    print("=" * 60)
    print(f"ğŸš€ ë¬´ì‹ ì‚¬ í¬ë¡¤ëŸ¬ v4 (ì•ˆì „ ìˆ˜ì§‘) ì‹œì‘")
    print(f"   ê²€ìƒ‰ì–´: {keyword}")
    print(f"   ëª©í‘œ: {max_products}ê°œ")
    print("=" * 60)
    
    # ì„¸ì…˜ ìƒì„± (ì¿ í‚¤ ìœ ì§€)
    session = requests.Session()
    
    # ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ
    collected_ids = load_progress()
    if collected_ids:
        print(f"ğŸ“‚ ì´ì „ ì§„í–‰ ìƒíƒœ ë³µì›: {len(collected_ids)}ê°œ ì´ë¯¸ ìˆ˜ì§‘ë¨")
    
    # 1ë‹¨ê³„: ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘
    print(f"\nğŸ” [1ë‹¨ê³„] ìƒí’ˆ ëª©ë¡ API í˜¸ì¶œ ì¤‘...")
    
    all_products = []
    page = 1
    
    while len(all_products) < max_products:
        products = get_product_list(keyword, page, BATCH_SIZE, session)
        
        if not products:
            print(f"   ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # ì´ë¯¸ ìˆ˜ì§‘í•œ í•­ëª© ì œì™¸
        new_products = [p for p in products if p.get('goodsNo') not in collected_ids]
        all_products.extend(new_products)
        
        print(f"   í˜ì´ì§€ {page}: {len(new_products)}ê°œ ì¶”ê°€ (ì´ {len(all_products)}ê°œ)")
        
        page += 1
        safe_delay(API_DELAY)
        
        if len(all_products) >= max_products:
            all_products = all_products[:max_products]
            break
    
    print(f"   âœ… ìˆ˜ì§‘í•  ìƒí’ˆ: {len(all_products)}ê°œ")
    
    # 2ë‹¨ê³„: íŒë§¤ì ì •ë³´ ìˆ˜ì§‘
    print(f"\nğŸ“¦ [2ë‹¨ê³„] íŒë§¤ì ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    
    results = []
    total = len(all_products)
    
    for idx, product in enumerate(all_products):
        goods_no = product.get("goodsNo")
        goods_name = product.get("goodsName", "")[:30]
        
        # ì§„í–‰ë¥  í‘œì‹œ
        progress = (idx + 1) / total * 100
        print(f"   [{idx+1}/{total}] ({progress:.1f}%) {goods_name}...")
        
        # íŒë§¤ì ì •ë³´ ì¶”ì¶œ
        seller_info = get_seller_info(goods_no, session)
        
        # ê²°ê³¼ ì¡°í•©
        result = {
            "goodsNo": goods_no,
            "title": product.get("goodsName"),
            "brand": product.get("brandName"),
            "price": product.get("price"),
            "normalPrice": product.get("normalPrice"),
            "saleRate": product.get("saleRate"),
            "url": f"https://www.musinsa.com/products/{goods_no}",
            "thumbnail": product.get("thumbnail"),
            "seller_info": seller_info
        }
        results.append(result)
        
        # ì§„í–‰ ìƒíƒœ ì €ì¥ (100ê°œë§ˆë‹¤)
        collected_ids.add(goods_no)
        if (idx + 1) % 100 == 0:
            save_progress(collected_ids)
            print(f"   ğŸ’¾ ì§„í–‰ ìƒíƒœ ì €ì¥ ì™„ë£Œ ({idx+1}ê°œ)")
        
        # ë”œë ˆì´
        safe_delay(HTML_DELAY)
    
    # ìµœì¢… ì €ì¥
    elapsed = time.time() - start_time
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_FILE.format(keyword=keyword, timestamp=timestamp)
    
    os.makedirs('data', exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"   ì´ ìˆ˜ì§‘: {len(results)}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("=" * 60)
    
    return results


if __name__ == "__main__":
    import sys
    keyword = sys.argv[1] if len(sys.argv) > 1 else SEARCH_KEYWORD
    max_items = int(sys.argv[2]) if len(sys.argv) > 2 else MAX_PRODUCTS
    run_crawler(keyword=keyword, max_products=max_items)
