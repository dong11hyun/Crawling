# 👨‍💻 핵심 코드 & 기술 리뷰 (Code Review Notes)

> 실리콘밸리 기술 면접이나 코드 리뷰 시 활용할 수 있는 **핵심 설계 철학**과 **구현 디테일**을 정리했습니다.
> 단순한 기능 구현을 넘어, **"왜(Why)"** 이 기술을 선택했고 **"어떻게(How)"** 최적화했는지에 집중합니다.

---

## 1. 🛡️ 크롤링 (Crawling): 안전과 속도의 균형

**핵심 질문**: "어떻게 하면 봇 탐지를 피하면서도 10배 빠르게 수집할 수 있는가?"

### 설계 철학 (Design Philosophy)
1.  **Hybrid Architecture (API + HTML)**:
    *   **문제**: Selenium/Playwright 같은 풀 브라우저 방식은 리소스 소모가 크고 느림 (페이지당 3~5초).
    *   **해결**: 상품 목록은 무신사 내부 API를 역공학하여 빠르게 가져오고(Batch), 상세 데이터만 HTML 파싱을 수행하는 하이브리드 전략 채택.
    *   **성과**: 속도 10배 향상 (분당 100건 → 1,000건).

2.  **Politeness Policy (정중함의 미학)**:
    *   **문제**: 과도한 요청은 429 (Too Many Requests) 에러나 IP 차단을 유발.
    *   **해결**: `Adaptive Throttling` (랜덤 딜레이)과 `User-Agent Rotation`을 적용하여 실제 사용자처럼 위장.

### 핵심 코드 (Implementation Detail)

```python
# src/v4_safe_crawler.py

# 1. User-Agent Rotation: 매 요청마다 새로운 브라우저인 척 위장
def get_headers():
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Referer": "https://www.musinsa.com/",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
    }

# 2. Adaptive Throttling: 기계적인 패턴을 숨기기 위한 랜덤 딜레이
def safe_delay(min_seconds=0.5, max_seconds=1.5):
    sleep_time = random.uniform(min_seconds, max_seconds)
    time.sleep(sleep_time)

# 3. Graceful Shutdown & Global State: 언제든 안전하게 멈출 수 있는 제어권
def run_crawler(...):
    global STOP_CRAWLER_FLAG, CRAWL_PROGRESS
    # ...
    for idx, product in enumerate(all_products):
        if STOP_CRAWLER_FLAG:
            print("🛑 사용자 요청으로 안전 종료 (Graceful Shutdown)")
            # ★ 중요: 지금까지 수집한 데이터를 저장하고 종료 (데이터 유실 방지)
            break
```
### 최적화 (Optimization) [NEW]
**"네트워크가 쉴 때 CPU도 쉬게 하자"**
1.  **lxml Parser**: C언어 기반 파서 도입. 복잡한 DOM 트리 분석 속도를 5배 향상 시켜 CPU 병목 제거.
    *   *Stability*: `lxml` 로드 실패 시 `html.parser`로 자동 전환되는 Fallback 로직 구현.
2.  **Bulk Indexing**: OpenSearch 입장에서 가장 비싼 연산은 '연결 수립'입니다.
    *   데이터를 20개씩 모아서 `helpers.bulk()`로 한 번에 보냅니다. (I/O 비용 1/20 감소)
3.  **Structured Logging**: `print` 디버깅 대신 Python `logging` 모듈로 교체.
    *   `TimedRotatingFileHandler`를 사용하여 날짜별 로그 파일 로테이션 구현.
    *   에러 발생 시 `exc_info=True`로 스택 트레이스 자동 기록.

---

## 2. ⚡ 캐싱 (Caching): 속도와 효율성

**핵심 질문**: "똑같은 검색 요청에 대해 왜 매번 무거운 DB 쿼리를 날려야 하는가?"

### 설계 철학 (Design Philosophy)
1.  **Cache-Aside Pattern**:
    *   데이터 읽기 시: 캐시 확인 → (없으면) DB 조회 → 캐시 저장 → 반환.
    *   이점: 구현이 단순하고, 캐시가 죽어도 DB에서 데이터를 가져올 수 있어 가용성(Availability)이 높음.
2.  **TTL (Time-To-Live)**:
    *   영구 저장하지 않고 5분(`300초`) 뒤 자동 만료.
    *   이유: 상품 가격이나 재고는 변하기 떄문에, 실시간성(Consistency)과 성능(Performance) 사이의 트레이드오프 조절.

### 핵심 코드 (Implementation Detail)

```python
# src/cache.py

# 1. 정교한 Cache Key 생성: 모든 파라미터가 키에 포함되어야 함
def generate_cache_key(prefix, **kwargs):
    # 예: search:패딩:10000:none (검색어:최소가:최대가)
    parts = [prefix]
    for key, value in sorted(kwargs.items()):
        parts.append(str(value) if value else "none")
    return ":".join(parts)

# src/api_server.py
@app.get("/search")
def search_products(...):
    # 2. 캐시 조회 (Hit)
    cache_key = generate_cache_key("search", keyword=keyword, ...)
    cached = get_cache(cache_key)
    if cached:
        return cached  # 🚀 OpenSearch 부하 0

    # 3. 캐시 미스 (Miss) -> DB 조회 -> 캐시 적재
    result = client.search(...) 
    set_cache(cache_key, result, ttl=300)
    return result
```

---

## 3. 🔍 오픈서치 (OpenSearch): 검색 엔진의 핵심

**핵심 질문**: "RDB(MySQL)의 `LIKE %검색어%` 대신 왜 검색 엔진을 써야 하는가?"

### 설계 철학 (Design Philosophy)
1.  **Inverted Index (역색인)**:
    *   책의 '맨 뒤 색인'처럼 단어별로 문서 위치를 미리 저장해둠.
    *   RDB는 모든 행을 스캔(Full Table Scan)해야 하지만, 검색 엔진은 O(1)에 가깝게 찾아냄.
2.  **형태소 분석 (Morphological Analysis)**:
    *   한국어 특성상 "패딩조끼"를 검색하면 "패딩"도 나오고 "조끼"도 나와야 함.
    *   기본적인 띄어쓰기 기준(Whitespace Analyzer)으로는 불가능.
    *   **Nori Tokenizer**를 도입하여 합성어를 분해(`decompound_mode: mixed`)함.

### 핵심 코드 (Implementation Detail)

```python
# src/init_opensearch.py

"settings": {
    "analysis": {
        "tokenizer": {
            "nori_user_dict": {
                "type": "nori_tokenizer",
                # ★ 핵심: 복합명사 분해 (예: 여성패딩 -> 여성, 패딩, 여성패딩)
                # 이를 통해 '여성'만 검색해도, '패딩'만 검색해도 노출됨
                "decompound_mode": "mixed" 
            }
        }
    }
}
```

---

## 4. 🚀 FastAPI: 현대적인 백엔드 설계

**핵심 질문**: "왜 Django나 Flask가 아닌 FastAPI인가?"

### 설계 철학 (Design Philosophy)
1.  **Asynchronous I/O (비동기 처리)**:
    *   Python은 기본적으로 느리지만(GIL), `async / await`를 사용하면 I/O 대기 시간(네트워크 요청, DB 쿼리) 동안 다른 작업을 처리할 수 있음.
    *   Node.js나 Go와 유사한 높은 동시성(Concurrency) 처리 가능.
2.  **Type Safety (Pydantic)**:
    *   런타임 에러를 획기적으로 줄이고, 자동완성 지원으로 생산성을 높임.
    *   데이터 유효성 검사(Validation)가 자동으로 수행되어 `if data is None:` 같은 방어 코드를 짤 필요가 없음.
3.  **Auto Documentation (Swagger UI)**:
    *   코드 자체가 문서가 됨. 프론트엔드 개발자에게 따로 엑셀로 API 명세서를 줄 필요가 없음 (`/docs`).

### 핵심 코드 (Implementation Detail)

```python
# src/api_server.py

# 1. Pydantic 모델: 엄격한 데이터 스키마 정의
class CrawlRequest(BaseModel):
    keyword: str
    max_products: int = 100  # 기본값 자동 설정 및 타입 강제

# 2. Dependency Injection & Background Tasks
@app.post("/crawl")
def trigger_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    # ★ 비동기 철학: 사용자에게는 "시작됨"만 응답하고(즉시 반환),
    # 실제 무거운 크롤링 작업은 백그라운드 큐에 던져둠.
    # 메인 스레드는 블로킹되지 않으므로 다른 API 요청을 계속 받을 수 있음.
    background_tasks.add_task(run_crawler, ...)
    return {"status": "processing"}
```

---

## 5. 🧠 심층 기술 개념 (Deep Dive Concepts)

> 각 기술 스택의 **표면 아래에 숨겨진 원리**를 이해합니다.

---

### 5.1 🕷️ 크롤링 심층 개념

#### HTTP 프로토콜 이해
```
Client (크롤러)  →  Request  →  Server (무신사)
                 ←  Response ←
```
- **Request**: Method(GET/POST), Headers, Body
- **Response**: Status Code(200, 403, 429), Headers, Body(HTML/JSON)
- 429 (Too Many Requests): Rate Limit 초과 시 반환

#### 동시성 모델 비교
| 방식 | 개념 | 적용 | 사용 시점 |
|------|------|------|----------|
| **동기 (Sync)** | 한 번에 하나씩 순차 처리 | `requests` | 단순 스크립트 |
| **비동기 (Async)** | I/O 대기 중 다른 작업 수행 | `aiohttp`, `asyncio` | 대량 요청 |
| **멀티스레딩** | 여러 스레드가 동시 실행 | `ThreadPoolExecutor` | I/O Bound |
| **멀티프로세싱** | 여러 프로세스가 병렬 실행 | `ProcessPoolExecutor` | CPU Bound |

#### I/O Bound vs CPU Bound
```
크롤링 = I/O Bound (네트워크 대기 시간이 대부분)
       → 비동기/멀티스레딩이 효과적
       → CPU 코어 늘려봤자 의미 없음

이미지 처리 = CPU Bound (연산이 대부분)
       → 멀티프로세싱이 효과적
       → 비동기는 의미 없음
```

#### 봇 탐지 & 우회 기법
| 탐지 방식 | 대응 전략 |
|-----------|----------|
| User-Agent 검사 | 브라우저 User-Agent 설정 |
| Rate Limiting | 요청 간 딜레이, 랜덤 간격 |
| IP 차단 | 프록시 로테이션 |
| JavaScript 렌더링 | Selenium, Playwright 사용 |
| Cookie/Session 검사 | 세션 유지 |

#### Robots.txt와 크롤링 윤리
```
/robots.txt → 크롤링 허용/금지 경로 명시
법적으로 구속력 없지만, 윤리적 기준으로 존중해야 함
과도한 크롤링은 서비스 장애를 유발할 수 있음
```

---

### 5.2 🔍 OpenSearch 심층 개념

#### 역색인 (Inverted Index) 원리
```
[일반 DB 방식]
문서1: "노스페이스 패딩 점퍼"
문서2: "아디다스 운동화"
문서3: "노스페이스 플리스"

검색: "노스페이스" → 모든 문서 전체 스캔 (O(n))

[역색인 방식]
"노스페이스" → [문서1, 문서3]
"패딩"      → [문서1]
"아디다스"  → [문서2]

검색: "노스페이스" → 즉시 [문서1, 문서3] 반환 (O(1))
```

#### 분석기 (Analyzer) 파이프라인
```
입력: "The Quick Brown Fox"
      ↓
Character Filter: 특수문자 제거, HTML 태그 제거
      ↓
Tokenizer: ["The", "Quick", "Brown", "Fox"]
      ↓
Token Filter (Lowercase): ["the", "quick", "brown", "fox"]
      ↓
Token Filter (Stop words): ["quick", "brown", "fox"]
      ↓
최종 색인: quick, brown, fox
```

#### 매핑 (Mapping) 타입 비교
| 타입 | 용도 | 분석 여부 | 집계 가능 |
|------|------|----------|----------|
| `text` | 전문 검색 (Full-text) | ✅ 분석됨 | ❌ |
| `keyword` | 정확한 값 매칭 | ❌ 분석 안됨 | ✅ |
| `integer` | 숫자, 범위 쿼리 | - | ✅ |
| `date` | 날짜, 시계열 | - | ✅ |

#### 샤드 & 레플리카 아키텍처
```
Index (musinsa_products)
├── Primary Shard 0  ←→  Replica 0  (다른 노드)
├── Primary Shard 1  ←→  Replica 1  (다른 노드)
└── Primary Shard 2  ←→  Replica 2  (다른 노드)

샤드 (Shard): 데이터를 수평 분할 → 확장성
레플리카 (Replica): 샤드의 복제본 → 고가용성 + 읽기 성능
```

#### Aggregation (집계) 구조
```
SQL과 비교:
SELECT brand, COUNT(*), AVG(price)
FROM products
GROUP BY brand
ORDER BY COUNT(*) DESC
LIMIT 10

↓ OpenSearch Query DSL

{
  "size": 0,
  "aggs": {
    "brands": {
      "terms": {
        "field": "brand",
        "size": 10
      },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    }
  }
}
```

#### 점수 계산 (Scoring) - BM25 알고리즘
```
TF (Term Frequency): 문서 내 단어 빈도수
  → "패딩"이 3번 나오면 1번 나온 문서보다 관련성 높음

IDF (Inverse Document Frequency): 전체 문서에서 희귀할수록 높음
  → "the"는 모든 문서에 있으므로 의미 없음
  → "노스페이스"는 일부 문서에만 있으므로 중요

BM25 = TF-IDF 개선 버전 (OpenSearch 기본 알고리즘)
  → 문서 길이 정규화 추가
  → 포화 함수 적용 (빈도가 너무 높아도 점수 제한)
```

---

### 5.3 ⚡ API 서버 (FastAPI) 심층 개념

#### ASGI vs WSGI 비교
```
WSGI (Flask, Django 기본):
  - 동기 방식
  - 요청당 스레드/프로세스 할당
  - 동시 1000명 → 1000개 스레드 필요

ASGI (FastAPI, Starlette):
  - 비동기 방식
  - 이벤트 루프 기반
  - 동시 1000명 → 싱글 스레드로 처리 가능
```

#### async/await 동작 원리
```python
async def fetch_data():
    # await 만나면 → 이벤트 루프에 제어권 넘김
    result = await http_client.get(url)  # I/O 대기
    # 대기하는 동안 다른 요청 처리 가능
    return result

# 이벤트 루프 내부:
# 1. 요청A 처리 시작 → await 만남 → 대기열에 등록
# 2. 요청B 처리 시작 → await 만남 → 대기열에 등록
# 3. 요청A의 I/O 완료 → 이어서 처리
# 4. 요청B의 I/O 완료 → 이어서 처리
```

#### Pydantic 타입 검증 흐름
```python
class Product(BaseModel):
    name: str           # 필수, 문자열
    price: int          # 필수, 정수
    brand: str | None   # 선택, 문자열 or None

# 요청 데이터:
# {"name": "패딩", "price": "50000"}  ← price가 문자열!

# Pydantic 자동 변환:
# {"name": "패딩", "price": 50000}   ← 정수로 변환됨

# 잘못된 요청:
# {"name": "패딩", "price": "비싸요"}  ← 변환 불가
# → 422 Unprocessable Entity 자동 반환
```

#### 의존성 주입 (Dependency Injection) 패턴
```python
# 의존성 정의
def get_opensearch_client():
    client = OpenSearch(hosts=[...])
    try:
        yield client  # 요청 처리 중 사용
    finally:
        client.close()  # 요청 완료 후 정리

# 사용
@app.get("/products")
async def list_products(client = Depends(get_opensearch_client)):
    return client.search(...)

# 장점:
# 1. 리소스 생명주기 자동 관리
# 2. 테스트 시 Mock으로 쉽게 교체
# 3. 코드 재사용성
```

#### REST API 설계 원칙
| 원칙 | 설명 | 예시 |
|------|------|------|
| **Stateless** | 서버가 클라이언트 상태 저장 X | 매 요청에 인증 토큰 포함 |
| **Uniform Interface** | 일관된 URL 구조 | `/products`, `/products/{id}` |
| **Resource-based** | URL은 명사 (동사 X) | ✅ `/products` ❌ `/getProducts` |
| **HTTP Method** | 동작은 메서드로 표현 | GET=조회, POST=생성, PUT=수정, DELETE=삭제 |

---

### 5.4 📊 대시보드 (시각화) 심층 개념

#### 집계 쿼리 실행 흐름
```
1. Dashboard UI
   └── 사용자 설정: Terms, field=brand, size=10
           ↓
2. Query DSL 생성
   └── POST musinsa_products/_search
       { "aggs": { "terms": { "field": "brand", "size": 10 } } }
           ↓
3. OpenSearch 클러스터
   └── 코디네이터 노드 → 각 샤드에 쿼리 분배
   └── 각 샤드: 로컬 집계 수행
   └── 코디네이터: 결과 병합 (Reduce)
           ↓
4. JSON Response
   └── { "aggregations": { "brands": { "buckets": [...] } } }
           ↓
5. Dashboard UI 렌더링
   └── D3.js / Canvas / SVG로 차트 생성
```

#### 시각화 유형별 적합한 데이터
| 차트 유형 | 적합한 데이터 | 예시 |
|----------|--------------|------|
| **Pie/Donut** | 구성 비율 | 브랜드별 점유율 |
| **Bar** | 비교 | 브랜드별 상품 수 비교 |
| **Line** | 시계열 | 일별 크롤링 수 추이 |
| **Histogram** | 분포 | 가격대별 상품 수 |
| **Data Table** | 상세 목록 | 상위 판매자 리스트 |
| **Metric** | 핵심 KPI | 총 상품 수, 평균 가격 |
| **Tag Cloud** | 빈도 시각화 | 브랜드명 워드클라우드 |

#### 집계 알고리즘 시간 복잡도
```
Terms Aggregation: O(n log k)
  n = 전체 문서 수
  k = size (상위 k개)
  → 힙(Heap) 자료구조 사용

Cardinality (유니크 값 개수): O(n)
  → HyperLogLog 알고리즘 사용
  → 정확도 vs 성능 트레이드오프 (약 2% 오차)

Date Histogram: O(n)
  → 시간 버킷별로 문서 그룹핑
```

#### 분산 집계 (Distributed Aggregation) 과정
```
[3개 샤드 환경에서 "상위 10개 브랜드" 조회]

1단계: 각 샤드에서 로컬 상위 10개 계산
  Shard0: [A:100, B:80, C:60, ...]
  Shard1: [A:50, D:90, E:70, ...]
  Shard2: [B:120, A:30, F:40, ...]

2단계: 코디네이터 노드에서 병합
  A: 100+50+30 = 180
  B: 80+120 = 200
  D: 90
  ...

3단계: 최종 정렬 후 상위 10개 반환

⚠️ 주의: 각 샤드 상위 10개만 가져오므로, 
    분산된 데이터의 합산 결과가 부정확할 수 있음
    해결: shard_size 파라미터로 더 많이 가져오기
```

---

## 🔗 전체 아키텍처 흐름도

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   무신사     │────▶│   크롤러     │────▶│  OpenSearch │
│   웹사이트   │ HTTP│  (Python)   │ Bulk│   클러스터   │
└─────────────┘     └─────────────┘     └──────┬──────┘
                                               │
                    ┌─────────────┐            │
                    │  API 서버   │◀───────────┤ Query
                    │  (FastAPI)  │            │
                    └──────┬──────┘            │
                           │                   │
                    ┌──────▼──────┐     ┌──────▼──────┐
                    │   프론트엔드  │     │  Dashboard  │
                    │   (React)   │     │ (OpenSearch │
                    │             │     │  Dashboards)│
                    └─────────────┘     └─────────────┘
```

---

## 🌟 결론 (Summary)

이 프로젝트는 단순한 "데이터 수집기"가 아닙니다.
**대규모 트래픽**과 **데이터 무결성**을 고려한 엔터프라이즈급 아키텍처의 축소판입니다.

1.  **확장성**: 하이브리드 크롤러 구조로 수집 속도와 범위를 확보했습니다.
2.  **안정성**: Graceful Shutdown과 이중 저장소(JSONL + OpenSearch)로 데이터 유실을 방지했습니다.
3.  **성능**: 캐싱(Redis)과 역색인(OpenSearch)을 통해 검색 Latency를 최소화했습니다.
4.  **관측 가능성**: 구조화된 로깅과 날짜별 파일 로테이션으로 운영 환경 대비 기반을 마련했습니다.

### 향후 확장 계획 (Future Work)
*   **Distributed Crawling**: Celery + RabbitMQ를 도입하여 대규모 분산 수집 지원.
*   **Data Integrity**: Pydantic 기반 엄격한 스키마 검증 파이프라인 구축.
*   **ELK Stack**: 중앙집중식 로그 수집 및 Kibana 대시보드 연동.
