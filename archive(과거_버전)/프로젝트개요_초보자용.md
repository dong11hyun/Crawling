# 🚀 프로젝트 개요: 무신사 크롤링 데이터 파이프라인
> **For Beginners** - 개발을 처음 시작하는 분들을 위한 가이드

---

## 📌 이 프로젝트가 뭐예요?

**무신사(Musinsa)** 라는 패션 쇼핑몰에서 상품 데이터를 **자동으로 수집하고**, **저장하고**, **검색할 수 있게** 만드는 프로젝트입니다.

### 🎯 한 줄 요약
```
무신사 웹사이트 → 크롤러가 데이터 수집 → 데이터베이스 저장 → 검색 API 제공
```

---

## 🏗️ 프로젝트 구조 (전체 그림)

```
┌─────────────────┐      ┌──────────────────┐      ┌─────────────────┐
│   무신사 웹사이트  │  →   │   Python 크롤러   │  →   │     데이터베이스    │
│   (데이터 원본)   │      │   (데이터 수집기)   │      │    (데이터 저장소)   │
└─────────────────┘      └──────────────────┘      └─────────────────┘
                                                           │
                                                           ▼
┌─────────────────┐      ┌──────────────────┐      ┌─────────────────┐
│    사용자/브라우저  │  ←   │    FastAPI 서버   │  ←   │   검색 엔진 (검색)   │
│  (Swagger UI)   │      │  (REST API 제공)   │      │   (OpenSearch)  │
└─────────────────┘      └──────────────────┘      └─────────────────┘
```

---

## 📁 폴더 구조

```
c:\Crawling\
│
├── 📁 src/                    # ⭐ 핵심 소스코드 (여기만 봐도 됨!)
│   ├── api_server.py         # 🌐 API 서버 (검색 기능)
│   ├── v2.2_crawler.py       # 🕷️ 크롤러 (데이터 수집)
│   ├── cache.py              # ⚡ 캐시 처리 (Redis 연동)
│   ├── database/             # 💾 DB 연결 및 모델 정의
│   ├── kafka_client/         # 📨 실시간 메시지 처리
│   ├── routers/              # 🔀 API 라우터 (CRUD)
│   └── tasks/                # ⏰ Airflow 태스크 모듈
│
├── 📁 airflow/               # 🔄 워크플로우 자동화
│   └── dags/                 # DAG 정의 파일
│
├── 📁 frontend/              # 🎨 웹 UI
│
├── docker-compose.yml        # 🐳 Docker 서비스 설정
├── requirements.txt          # 📦 Python 패키지 목록
│
├── WEEK_1_실행방법.md        # 📖 Week 1 가이드
├── WEEK_2_실행방법.md        # 📖 Week 2 가이드
└── WEEK_3_실행방법.md        # 📖 Week 3 가이드
```

---

## 🛠️ 사용된 기술 스택 (쉬운 설명)

### 1️⃣ 프로그래밍 언어

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Python** | 읽기 쉽고 배우기 쉬운 프로그래밍 언어 | 크롤러, API 서버, 모든 로직 작성 |

---

### 2️⃣ 웹 크롤링 (데이터 수집)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Playwright** | 브라우저를 자동으로 조종하는 도구 | 무신사 웹페이지를 열고, 상품 정보를 긁어옴 |

> 💡 **예시**: 내가 직접 마우스로 클릭하고 스크롤하지 않아도, Playwright가 대신 해줌!

---

### 3️⃣ 데이터베이스 (데이터 저장)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **PostgreSQL** | 관계형 데이터베이스 (엑셀처럼 표로 저장) | 상품의 원본 데이터를 안전하게 저장 |
| **SQLAlchemy** | Python에서 DB를 쉽게 다루는 도구 | SQL 문법 없이 Python 코드로 DB 조작 |

> 💡 **비유**: PostgreSQL = 파일 캐비닛, SQLAlchemy = 캐비닛을 정리해주는 비서

---

### 4️⃣ 검색 엔진

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **OpenSearch** | 빠른 검색을 위한 전문 검색 엔진 | "패딩"을 검색하면 관련 상품을 0.1초 안에 찾아줌 |

> 💡 **비유**: 네이버/구글 검색처럼 검색어를 입력하면 빠르게 결과를 반환

---

### 5️⃣ API 서버 (데이터 제공)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **FastAPI** | Python으로 API를 빠르게 만드는 프레임워크 | `/search?keyword=패딩` 같은 요청을 처리 |
| **Pydantic** | 데이터 검증 라이브러리 | 들어온 데이터가 올바른지 체크 |

> 💡 **비유**: FastAPI = 음식점 웨이터 (주문 받고 음식 전달)

---

### 6️⃣ 캐시 (속도 향상)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Redis** | 초고속 임시 저장소 (메모리 DB) | 자주 검색되는 결과를 저장해두고 재사용 |

> 💡 **비유**: 
> - Redis 없음: 매번 창고까지 가서 물건 찾기 (느림)
> - Redis 있음: 자주 쓰는 물건은 책상 위에 (빠름)

---

### 7️⃣ 메시지 큐 (실시간 처리)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Kafka** | 대용량 메시지 전달 시스템 | 크롤러 → Kafka → DB로 실시간 데이터 전송 |
| **Zookeeper** | Kafka를 관리하는 코디네이터 | Kafka가 잘 돌아가게 관리 |

> 💡 **비유**: Kafka = 택배 허브 (물건을 모아서 각 목적지로 배송)

---

### 8️⃣ 워크플로우 자동화

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Airflow** | 작업 자동화/스케줄링 도구 | "매일 오전 9시에 크롤링 실행" 같은 자동화 |

> 💡 **비유**: 알람시계 + 비서 (정해진 시간에 정해진 일을 알아서 함)

---

### 9️⃣ 컨테이너 (환경 구성)

| 기술 | 뭐예요? | 이 프로젝트에서 하는 일 |
|------|---------|-------------------------|
| **Docker** | 앱을 상자에 담아 실행하는 기술 | PostgreSQL, Redis 등을 한 번에 설치 없이 실행 |
| **Docker Compose** | 여러 Docker 컨테이너를 한 번에 관리 | `docker-compose up` 한 줄로 모든 서비스 시작 |

> 💡 **비유**: Docker = 레고 블록 (조립하면 바로 사용 가능)

---

## 🔄 데이터 흐름도 (Week별)

### Week 1: 기본 백엔드
```
[무신사] → [Playwright 크롤러] → [PostgreSQL] + [OpenSearch] → [FastAPI] → [사용자]
                                         ↓
                                    [Redis 캐시]
```

### Week 2: Airflow 자동화
```
[Airflow 스케줄러]
        │
        ▼
[crawl_task] → [validate_task] → [load_task]
(크롤링)         (검증)           (저장)
```

### Week 3: Kafka 실시간 파이프라인
```
[크롤러] → [Kafka Producer] → [Kafka] → [Consumer] → [PostgreSQL]
                                 │                         
                                 └──────→ [Consumer] → [OpenSearch]
```

---

## 🌐 접속 주소 요약

| 서비스 | 주소 | 설명 |
|--------|------|------|
| FastAPI Swagger | http://localhost:8000/docs | API 테스트 UI |
| pgAdmin | http://localhost:5050 | PostgreSQL 관리 |
| OpenSearch Dashboards | http://localhost:5601 | 검색 데이터 시각화 |
| Airflow | http://localhost:8080 | 워크플로우 관리 |
| Kafka UI | http://localhost:8088 | 메시지 모니터링 |

---

## 📚 공부 순서 추천

```
1️⃣ Python 기초
        ↓
2️⃣ FastAPI로 간단한 API 만들기
        ↓
3️⃣ PostgreSQL + SQLAlchemy 이해
        ↓
4️⃣ Docker로 환경 구성
        ↓
5️⃣ Playwright로 크롤링
        ↓
6️⃣ OpenSearch로 검색 기능
        ↓
7️⃣ Redis 캐싱 적용
        ↓
8️⃣ Airflow 자동화
        ↓
9️⃣ Kafka 실시간 처리
```

---

## ❓ 자주 묻는 질문

**Q: 왜 PostgreSQL과 OpenSearch 둘 다 사용하나요?**
> PostgreSQL: 원본 데이터 안전하게 보관 (CRUD)
> OpenSearch: 빠른 검색에 특화 (각각 장점이 다름!)

**Q: Redis가 없으면 안 되나요?**
> 없어도 동작함. 하지만 같은 검색을 100번 하면 100번 다 DB에 물어봐야 해서 느림.

**Q: Docker를 꼭 써야 하나요?**
> 안 써도 됨. 단, PostgreSQL, Redis, Kafka 등을 직접 설치해야 해서 매우 번거로움.

---

## 🎉 시작하기

```bash
# 1. 프로젝트 폴더로 이동
cd c:\Crawling

# 2. 모든 서비스 시작 (Docker 필요)
docker-compose up -d

# 3. FastAPI 서버 실행
cd src
python api_server.py

# 4. 브라우저에서 테스트
# http://localhost:8000/docs 접속
```

---

> 💬 **Tip**: 처음엔 모든 걸 이해하려 하지 마세요. Week 1부터 차근차근 따라하면 자연스럽게 이해됩니다! 🌱
