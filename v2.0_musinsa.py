import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import random

# ==========================================
# [ì„¤ì •] ìŠ¤í¬ë¡¤ íšŸìˆ˜ ë° ì €ì¥ íŒŒì¼ëª…
# ==========================================
SEARCH_KEYWORD = "íŒ¨ë”©"
TARGET_URL = f"https://www.musinsa.com/search/goods?keyword={SEARCH_KEYWORD}&gf=A"
SCROLL_COUNT = 5  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5ë²ˆë§Œ ìŠ¤í¬ë¡¤ (ì›í•˜ëŠ” ë§Œí¼ ëŠ˜ë¦¬ì„¸ìš”)
OUTPUT_FILE = "musinsa_padding_info.csv"

async def run():
    async with async_playwright() as p:
        # 1. ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless=Falseë¡œ í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ëœ¨ëŠ”ê²Œ ë³´ì„)
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        # -------------------------------------------------------
        # 1ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ URL ìˆ˜ì§‘ (ë¬´í•œ ìŠ¤í¬ë¡¤)
        # -------------------------------------------------------
        print(f"ğŸš€ [1ë‹¨ê³„] '{SEARCH_KEYWORD}' ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘...")
        await page.goto(TARGET_URL, timeout=60000)
        await page.wait_for_load_state("networkidle")

        print(f"   â¬‡ï¸ ìŠ¤í¬ë¡¤ {SCROLL_COUNT}íšŒ ì§„í–‰ ì¤‘...")
        for i in range(SCROLL_COUNT):
            # ìŠ¤í¬ë¡¤ì„ ë°”ë‹¥ê¹Œì§€ ë‚´ë¦¼
            await page.keyboard.press("End")
            await asyncio.sleep(random.uniform(1.5, 3.0)) # ë¡œë”© ëŒ€ê¸° (ì‚¬ëŒì²˜ëŸ¼)
            print(f"   - ìŠ¤í¬ë¡¤ {i+1}/{SCROLL_COUNT} ì™„ë£Œ")

        # ìƒí’ˆ ë§í¬ ì¶”ì¶œ (href ì†ì„± ìˆ˜ì§‘)
        # ë¬´ì‹ ì‚¬ ìƒí’ˆ ë§í¬ëŠ” ë³´í†µ /products/ìˆ«ì í˜•íƒœì„
        print("   ğŸ” ìƒí’ˆ ë§í¬ ì¶”ì¶œ ì¤‘...")
        product_locators = page.locator("a[href*='/products/']")
        count = await product_locators.count()
        
        product_urls = set()
        for i in range(count):
            href = await product_locators.nth(i).get_attribute("href")
            if href:
                # httpsê°€ ì—†ëŠ” ìƒëŒ€ê²½ë¡œì¼ ê²½ìš° ì²˜ë¦¬
                if not href.startswith("http"):
                    href = "https://www.musinsa.com" + href
                product_urls.add(href)
        
        # ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        url_list = list(product_urls)
        print(f"   âœ… ì´ {len(url_list)}ê°œì˜ ìƒí’ˆ URL í™•ë³´ ì™„ë£Œ!")

        # -------------------------------------------------------
        # 2ë‹¨ê³„: ê° ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ë° ì •ë³´ ì¶”ì¶œ
        # -------------------------------------------------------
        print(f"\nğŸš€ [2ë‹¨ê³„] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ (ì´ {len(url_list)}ê°œ)")
        results = []

        for idx, url in enumerate(url_list):
            try:
                print(f"   [{idx+1}/{len(url_list)}] ì ‘ì† ì¤‘: {url}")
                await page.goto(url, timeout=30000)
                
                # 'íŒë§¤ì ì •ë³´' í…ìŠ¤íŠ¸ê°€ ë³´ì¼ ë•Œê¹Œì§€ ëŒ€ê¸° (í˜¹ì€ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°)
                try:
                    # ì•„ì½”ë””ì–¸ì´ ë‹«í˜€ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 'íŒë§¤ì ì •ë³´' ë²„íŠ¼ì„ ì°¾ì•„ í´ë¦­ ì‹œë„
                    seller_btn = page.locator("button:has-text('íŒë§¤ì ì •ë³´')")
                    if await seller_btn.count() > 0:
                        # aria-expandedê°€ falseë©´ í´ë¦­í•´ì„œ ì—´ê¸°
                        is_expanded = await seller_btn.get_attribute("aria-expanded")
                        if is_expanded == "false":
                            await seller_btn.click()
                            await asyncio.sleep(0.5)
                except:
                    pass # ì´ë¯¸ ì—´ë ¤ìˆê±°ë‚˜ ë²„íŠ¼ì´ ë‹¤ë¥¼ ê²½ìš° íŒ¨ìŠ¤

                # ë°ì´í„° ì¶”ì¶œ (XPath ì‚¬ìš© - í´ë˜ìŠ¤ëª…ì´ ë°”ë€Œì–´ë„ ëŒ€ì‘ ê°€ëŠ¥í•˜ë„ë¡)
                info = {
                    "ìƒí’ˆURL": url,
                    "ìƒí˜¸/ëŒ€í‘œì": await get_text_by_label(page, "ìƒí˜¸ / ëŒ€í‘œì"),
                    "ë¸Œëœë“œ": await get_text_by_label(page, "ë¸Œëœë“œ"),
                    "ì‚¬ì—…ìë²ˆí˜¸": await get_text_by_label(page, "ì‚¬ì—…ìë²ˆí˜¸"),
                    "ì—°ë½ì²˜": await get_text_by_label(page, "ì—°ë½ì²˜"),
                    "E-mail": await get_text_by_label(page, "E-mail"),
                    "ì˜ì—…ì†Œì¬ì§€": await get_text_by_label(page, "ì˜ì—…ì†Œì¬ì§€"),
                }
                
                print(f"      ğŸ‘‰ ìˆ˜ì§‘: {info['ìƒí˜¸/ëŒ€í‘œì']} / {info['ë¸Œëœë“œ']}")
                results.append(info)

                # ë´‡ íƒì§€ ë°©ì§€ìš© ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(random.uniform(1, 2))

            except Exception as e:
                print(f"      âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                continue

        # -------------------------------------------------------
        # 3ë‹¨ê³„: íŒŒì¼ ì €ì¥
        # -------------------------------------------------------
        if results:
            df = pd.DataFrame(results)
            df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
            print(f"\nğŸ‰ [ì™„ë£Œ] '{OUTPUT_FILE}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        await browser.close()

async def get_text_by_label(page, label_text):
    """
    <dt>ë¼ë²¨</dt><dd>ê°’</dd> êµ¬ì¡°ì—ì„œ ë¼ë²¨ í…ìŠ¤íŠ¸ë¡œ ê°’ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    try:
        # dt íƒœê·¸ ì¤‘ label_textë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œë¥¼ ì°¾ê³ , ê·¸ ë°”ë¡œ ë’¤ì˜ dd íƒœê·¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        locator = page.locator(f"//dt[contains(., '{label_text}')]/following-sibling::dd[1]")
        if await locator.count() > 0:
            return await locator.inner_text()
        return "-"
    except:
        return "-"

if __name__ == "__main__":
    asyncio.run(run())